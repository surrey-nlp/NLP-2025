{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 02: POS Tagging, Morphology, Lemmatization, Dependency Parsing, and Tokenization\n",
    "\n",
    "## Introduction  \n",
    "\n",
    "**Natural Language Processing (NLP)** is a field of artificial intelligence that focuses on the interaction between computers and human languages. It involves the development of models that enable computers to process, analyze, and understand human language.  \n",
    "\n",
    "In this tutorial, we will explore some core NLP tasks using **spaCy**, a powerful and efficient Python library for NLP. Additionally, we will examine tokenization techniques used in modern language models.  \n",
    "\n",
    "### Topics Covered:  \n",
    "\n",
    "1. **Tokenization**  \n",
    "2. **Part-of-Speech (POS) Tagging**  \n",
    "3. **Lemmatization**  \n",
    "4. **Morphology**  \n",
    "5. **Dependency Parsing**  \n",
    "6. **Subword Tokenization**  \n",
    "\n",
    "## Prerequisites  \n",
    "\n",
    "Before we begin, ensure that you have **spaCy** installed in your environment. If you are using the `NL2025` environment, make sure it is activated. You can install **spaCy** using the following command:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.12-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.5 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.15.1-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/jl02958/miniconda3/envs/hamer_debug/lib/python3.10/site-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/jl02958/miniconda3/envs/hamer_debug/lib/python3.10/site-packages (from spacy) (1.26.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/jl02958/miniconda3/envs/hamer_debug/lib/python3.10/site-packages (from spacy) (2.28.1)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: jinja2 in /home/jl02958/miniconda3/envs/hamer_debug/lib/python3.10/site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /home/jl02958/miniconda3/envs/hamer_debug/lib/python3.10/site-packages (from spacy) (75.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jl02958/miniconda3/envs/hamer_debug/lib/python3.10/site-packages (from spacy) (24.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /home/jl02958/miniconda3/envs/hamer_debug/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/jl02958/miniconda3/envs/hamer_debug/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jl02958/miniconda3/envs/hamer_debug/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/jl02958/miniconda3/envs/hamer_debug/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jl02958/miniconda3/envs/hamer_debug/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/jl02958/miniconda3/envs/hamer_debug/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/jl02958/miniconda3/envs/hamer_debug/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.3)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.20.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jl02958/miniconda3/envs/hamer_debug/lib/python3.10/site-packages (from jinja2->spacy) (2.1.5)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/jl02958/miniconda3/envs/hamer_debug/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/jl02958/miniconda3/envs/hamer_debug/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading wrapt-1.17.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/jl02958/miniconda3/envs/hamer_debug/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Downloading spacy-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.2/29.2 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (204 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.12-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "Downloading preshed-3.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (156 kB)\n",
      "Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Downloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading thinc-8.3.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typer-0.15.1-py3-none-any.whl (44 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading blis-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cloudpathlib-0.20.0-py3-none-any.whl (52 kB)\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Downloading marisa_trie-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wrapt-1.17.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (82 kB)\n",
      "Installing collected packages: cymem, wrapt, wasabi, spacy-loggers, spacy-legacy, shellingham, pydantic-core, murmurhash, marisa-trie, cloudpathlib, catalogue, blis, annotated-types, srsly, smart-open, pydantic, preshed, language-data, typer, langcodes, confection, weasel, thinc, spacy\n",
      "Successfully installed annotated-types-0.7.0 blis-1.2.0 catalogue-2.0.10 cloudpathlib-0.20.0 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 murmurhash-1.0.12 preshed-3.0.9 pydantic-2.10.6 pydantic-core-2.27.2 shellingham-1.5.4 smart-open-7.1.0 spacy-3.8.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.4 typer-0.15.1 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.2\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, download the spaCy model for the English language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing spaCy\n",
    "Let's start by importing the spaCy library and loading the English language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before beginning, let's define an example string that we can look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The University of Surrey is a U.K. university founded in 1966, with a budget of £314.0 million.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenization  \n",
    "\n",
    "**Tokenization** is the process of breaking down text input into smaller units called **tokens**, which can be **words, punctuation marks, or other meaningful elements**. This is a fundamental step in **Natural Language Processing (NLP)** as it enables structured text analysis.  \n",
    "\n",
    "### Tokenization in spaCy  \n",
    "\n",
    "In **spaCy**, tokenization is performed using language-specific grammatical rules. For example:  \n",
    "- Punctuation at the end of a sentence is **split off** as a separate token.  \n",
    "- Abbreviations like **\"U.K.\"** retain their periods within a single token.  \n",
    "\n",
    "### How spaCy Handles Tokenization  \n",
    "\n",
    "- The **input** to the tokenizer is a **Unicode text**.  \n",
    "- The **output** is a **Doc object**, which consists of individual tokens.  \n",
    "- We can **iterate** over tokens and access attributes such as `token.text`.  \n",
    "- spaCy's tokenizer is **non-destructive**, meaning it preserves the original text while providing structured access to tokens.  \n",
    "\n",
    "This efficient tokenization process enables deeper linguistic analysis while maintaining the integrity of the original text.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Position  Text\n",
      "----------  ----------\n",
      "         1  The\n",
      "         2  University\n",
      "         3  of\n",
      "         4  Surrey\n",
      "         5  is\n",
      "         6  a\n",
      "         7  U.K.\n",
      "         8  university\n",
      "         9  founded\n",
      "        10  in\n",
      "        11  1966\n",
      "        12  ,\n",
      "        13  with\n",
      "        14  a\n",
      "        15  budget\n",
      "        16  of\n",
      "        17  £\n",
      "        18  314.0\n",
      "        19  million\n",
      "        20  .\n"
     ]
    }
   ],
   "source": [
    "# Useful Library for formatting the table\n",
    "import tabulate\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# Plot table \n",
    "table = []\n",
    "for count, token in enumerate(doc):\n",
    "    table.append([count + 1, token.text])\n",
    "\n",
    "print(tabulate.tabulate(table, headers=['Position','Text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Part-of-Speech (POS) Tagging  \n",
    "\n",
    "**Part-of-Speech (POS) tagging** is the process of assigning grammatical tags to individual words in a sentence, indicating their role, such as **noun, verb, adjective,** etc. This helps in understanding the **syntactic structure** of a sentence and is fundamental in many Natural Language Processing (NLP) tasks.  \n",
    "\n",
    "### Using spaCy for POS Tagging  \n",
    "\n",
    "Since we have previously processed the text input using **spaCy**, we can easily retrieve the POS tag for each token with a simple attribute call `token.pos_`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Position  Text        POS Tag\n",
      "----------  ----------  ---------\n",
      "         1  The         DET\n",
      "         2  University  PROPN\n",
      "         3  of          ADP\n",
      "         4  Surrey      PROPN\n",
      "         5  is          AUX\n",
      "         6  a           DET\n",
      "         7  U.K.        PROPN\n",
      "         8  university  NOUN\n",
      "         9  founded     VERB\n",
      "        10  in          ADP\n",
      "        11  1966        NUM\n",
      "        12  ,           PUNCT\n",
      "        13  with        ADP\n",
      "        14  a           DET\n",
      "        15  budget      NOUN\n",
      "        16  of          ADP\n",
      "        17  £           SYM\n",
      "        18  314.0       NUM\n",
      "        19  million     NUM\n",
      "        20  .           PUNCT\n"
     ]
    }
   ],
   "source": [
    "POS_Tags = []\n",
    "for count, token in enumerate(doc):\n",
    "    POS_Tags.append([count + 1, token.text, token.pos_])\n",
    "\n",
    "print(tabulate.tabulate(POS_Tags, headers=['Position','Text', 'POS Tag']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the example above, we can see several **POS Tags**. Some common examples include:\n",
    "\n",
    "- **DET**: Determiner  \n",
    "- **PROPN**: Proper Noun  \n",
    "- **ADP**: Adposition  \n",
    "\n",
    "These tags represent different parts of speech in a sentence and are crucial for understanding the syntactic structure of the language.\n",
    "\n",
    "### Why Use POS Tagging?\n",
    "\n",
    "In **Natural Language Processing (NLP)**, understanding the grammatical structure of sentences can be extremely valuable for many tasks. POS tagging helps computers to identify the roles that different words play within a sentence, such as subjects, objects, or actions.\n",
    "\n",
    "However, in some tasks, it may also be useful to **discard certain words** based on their POS tags. For example:\n",
    "\n",
    "- **Sentiment Analysis**:  \n",
    "  In sentiment analysis, words like **articles** (e.g., *\"the\"*, *\"a\"*) and **pronouns** (e.g., *\"he\"*, *\"she\"*) might be discarded because they contribute little to the overall sentiment of the text.\n",
    "\n",
    "By filtering out less relevant POS tags, the model can focus on words that carry more meaning and help improve task performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Lemmatization  \n",
    "\n",
    "**Lemmatization** is the process of reducing words to their **base** or **root** form, known as a **lemma**. This helps in **text normalization** by converting different inflectional forms of a word into a single standardized form.  \n",
    "\n",
    "Lemmatization is particularly useful in **Natural Language Processing (NLP)** tasks such as:  \n",
    "- Improving text **search and retrieval**  \n",
    "- Enhancing **sentiment analysis**  \n",
    "- Reducing **dimensionality** in text-based models  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Position  Text        Lemma\n",
      "----------  ----------  ----------\n",
      "         1  The         the\n",
      "         2  University  University\n",
      "         3  of          of\n",
      "         4  Surrey      Surrey\n",
      "         5  is          be\n",
      "         6  a           a\n",
      "         7  U.K.        U.K.\n",
      "         8  university  university\n",
      "         9  founded     found\n",
      "        10  in          in\n",
      "        11  1966        1966\n",
      "        12  ,           ,\n",
      "        13  with        with\n",
      "        14  a           a\n",
      "        15  budget      budget\n",
      "        16  of          of\n",
      "        17  £           £\n",
      "        18  314.0       314.0\n",
      "        19  million     million\n",
      "        20  .           .\n"
     ]
    }
   ],
   "source": [
    "Morphs = []\n",
    "for count, token in enumerate(doc):\n",
    "    Morphs.append([count + 1, token.text, token.lemma_])\n",
    "\n",
    "print(tabulate.tabulate(Morphs, headers=['Position','Text','Lemma']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Morphology  \n",
    "\n",
    "**Morphology** is the study of the structure of words and their components, such as **prefixes, suffixes,** and **roots**. In essence, it is the process through which the root form (lemma) of a word is modified by the addition of prefixes or suffixes, altering its meaning or grammatical function.  \n",
    "\n",
    "In **spaCy**, we can access detailed morphological information for each token, which includes features such as:  \n",
    "- **Number** (singular or plural)  \n",
    "- **Tense** (present, past, etc.)  \n",
    "- **Mood** – Indicates the mode or manner in which the action is expressed (e.g., **indicative**, **imperative**, or **subjunctive**).  \n",
    "  - Example: *\"She eats\"* (indicative) vs. *\"Eat!\"* (imperative)\n",
    "- **Aspect** – Describes the temporal flow or completion of an action (e.g., **perfective**, **progressive**, or **habitual**).  \n",
    "  - Example: *\"I am eating\"* (progressive) vs. *\"I have eaten\"* (perfective)  \n",
    "\n",
    "This morphological analysis is essential for understanding how words relate to one another in context and is crucial for tasks such as syntactic parsing and word generation.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Position  Text        Morphology\n",
      "----------  ----------  -----------------------------------------------------\n",
      "         1  The         Definite=Def|PronType=Art\n",
      "         2  University  Number=Sing\n",
      "         3  of\n",
      "         4  Surrey      Number=Sing\n",
      "         5  is          Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\n",
      "         6  a           Definite=Ind|PronType=Art\n",
      "         7  U.K.        Number=Sing\n",
      "         8  university  Number=Sing\n",
      "         9  founded     Aspect=Perf|Tense=Past|VerbForm=Part\n",
      "        10  in\n",
      "        11  1966        NumType=Card\n",
      "        12  ,           PunctType=Comm\n",
      "        13  with\n",
      "        14  a           Definite=Ind|PronType=Art\n",
      "        15  budget      Number=Sing\n",
      "        16  of\n",
      "        17  £\n",
      "        18  314.0       NumType=Card\n",
      "        19  million     NumType=Card\n",
      "        20  .           PunctType=Peri\n"
     ]
    }
   ],
   "source": [
    "Morphs = []\n",
    "for count, token in enumerate(doc):\n",
    "    Morphs.append([count + 1, token.text, token.morph])\n",
    "\n",
    "print(tabulate.tabulate(Morphs, headers=['Position','Text', 'Morphology']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dependency Parsing  \n",
    "\n",
    "Dependency parsing involves analyzing the grammatical structure of a sentence and establishing relationships between **head** words and their **modifiers**. This technique allows us to decompose a sentence into multiple sections, assuming a direct connection between each linguistic unit. These relationships are typically represented as a **tree structure**, illustrating how words depend on one another.  \n",
    "\n",
    "### Example  \n",
    "\n",
    "**Sentence:**  \n",
    "*\"I prefer the morning flight through Denver.\"*  \n",
    "\n",
    "The diagram below visualizes the sentence's dependency structure:  \n",
    "\n",
    "![Dependency Parsing](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/09/29920Screenshot-127.webp)  \n",
    "*[Source](https://www.analyticsvidhya.com/blog/2021/12/dependency-parsing-in-natural-language-processing-with-examples/)*  \n",
    "\n",
    "### Understanding the Dependency Structure  \n",
    "\n",
    "In the diagram:  \n",
    "\n",
    "- **Directed arcs** illustrate grammatical relationships between words in the sentence.  \n",
    "- The **root** of the tree, *prefer*, serves as the central unit of the sentence.  \n",
    "- Each dependency is labeled with a **dependency tag**, which specifies the relationship between two words.  \n",
    "\n",
    "For instance, in the phrase **\"flight to Denver\"**, the noun *Denver* modifies the meaning of *flight*. This creates a **dependency** where:  \n",
    "\n",
    "- *Flight* is the **head** (governing word).  \n",
    "- *Denver* is the **dependent** (child node).  \n",
    "- This relationship is marked by the **nmod** (nominal modifier) tag, indicating that *Denver* provides additional information about *flight*.  \n",
    "\n",
    "Dependency parsing plays a crucial role in natural language processing (NLP), helping models understand syntactic structures and improving tasks such as named entity recognition, question answering, and machine translation.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of this can be done easily with spaCy through the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Position  Text        Dependency    Children\n",
      "----------  ----------  ------------  -----------------------------------------\n",
      "         1  The         det           []\n",
      "         2  University  nsubj         ['The', 'of']\n",
      "         3  of          prep          ['Surrey']\n",
      "         4  Surrey      pobj          []\n",
      "         5  is          ROOT          ['University', 'university', 'with', '.']\n",
      "         6  a           det           []\n",
      "         7  U.K.        compound      []\n",
      "         8  university  attr          ['a', 'U.K.', 'founded', ',']\n",
      "         9  founded     acl           ['in']\n",
      "        10  in          prep          ['1966']\n",
      "        11  1966        pobj          []\n",
      "        12  ,           punct         []\n",
      "        13  with        prep          ['budget']\n",
      "        14  a           det           []\n",
      "        15  budget      pobj          ['a', 'of']\n",
      "        16  of          prep          ['million']\n",
      "        17  £           quantmod      []\n",
      "        18  314.0       compound      []\n",
      "        19  million     pobj          ['£', '314.0']\n",
      "        20  .           punct         []\n"
     ]
    }
   ],
   "source": [
    "Dependenct_Parsing = []\n",
    "for count, token in enumerate(doc):\n",
    "    Dependenct_Parsing.append([count + 1, token.text, token.dep_, [child.text for child in token.children]])\n",
    "\n",
    "print(tabulate.tabulate(Dependenct_Parsing, headers=['Position','Text', 'Dependency', 'Children']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table might look very confusing, which is why spaCy offers a quick way to easily view the tree structure with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"6d3fcfbb0d0945feb9519bc0c5c396f1-0\" class=\"displacy\" width=\"3200\" height=\"487.0\" direction=\"ltr\" style=\"max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">University</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">of</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">Surrey</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">U.K.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">university</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">founded</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">1966,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">with</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2325\">budget</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2325\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2500\">of</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2500\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2675\">£</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2675\">SYM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2850\">314.0</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2850\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3025\">million.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3025\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6d3fcfbb0d0945feb9519bc0c5c396f1-0-0\" stroke-width=\"2px\" d=\"M70,352.0 C70,264.5 210.0,264.5 210.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6d3fcfbb0d0945feb9519bc0c5c396f1-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,354.0 L62,342.0 78,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6d3fcfbb0d0945feb9519bc0c5c396f1-0-1\" stroke-width=\"2px\" d=\"M245,352.0 C245,177.0 740.0,177.0 740.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6d3fcfbb0d0945feb9519bc0c5c396f1-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,354.0 L237,342.0 253,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6d3fcfbb0d0945feb9519bc0c5c396f1-0-2\" stroke-width=\"2px\" d=\"M245,352.0 C245,264.5 385.0,264.5 385.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6d3fcfbb0d0945feb9519bc0c5c396f1-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M385.0,354.0 L393.0,342.0 377.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6d3fcfbb0d0945feb9519bc0c5c396f1-0-3\" stroke-width=\"2px\" d=\"M420,352.0 C420,264.5 560.0,264.5 560.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6d3fcfbb0d0945feb9519bc0c5c396f1-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M560.0,354.0 L568.0,342.0 552.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6d3fcfbb0d0945feb9519bc0c5c396f1-0-4\" stroke-width=\"2px\" d=\"M945,352.0 C945,177.0 1265.0,177.0 1265.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6d3fcfbb0d0945feb9519bc0c5c396f1-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,354.0 L937,342.0 953,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6d3fcfbb0d0945feb9519bc0c5c396f1-0-5\" stroke-width=\"2px\" d=\"M1120,352.0 C1120,264.5 1260.0,264.5 1260.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6d3fcfbb0d0945feb9519bc0c5c396f1-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,354.0 L1112,342.0 1128,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6d3fcfbb0d0945feb9519bc0c5c396f1-0-6\" stroke-width=\"2px\" d=\"M770,352.0 C770,89.5 1270.0,89.5 1270.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6d3fcfbb0d0945feb9519bc0c5c396f1-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1270.0,354.0 L1278.0,342.0 1262.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6d3fcfbb0d0945feb9519bc0c5c396f1-0-7\" stroke-width=\"2px\" d=\"M1295,352.0 C1295,264.5 1435.0,264.5 1435.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6d3fcfbb0d0945feb9519bc0c5c396f1-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1435.0,354.0 L1443.0,342.0 1427.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6d3fcfbb0d0945feb9519bc0c5c396f1-0-8\" stroke-width=\"2px\" d=\"M1470,352.0 C1470,264.5 1610.0,264.5 1610.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6d3fcfbb0d0945feb9519bc0c5c396f1-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1610.0,354.0 L1618.0,342.0 1602.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6d3fcfbb0d0945feb9519bc0c5c396f1-0-9\" stroke-width=\"2px\" d=\"M1645,352.0 C1645,264.5 1785.0,264.5 1785.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6d3fcfbb0d0945feb9519bc0c5c396f1-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1785.0,354.0 L1793.0,342.0 1777.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6d3fcfbb0d0945feb9519bc0c5c396f1-0-10\" stroke-width=\"2px\" d=\"M770,352.0 C770,2.0 1975.0,2.0 1975.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6d3fcfbb0d0945feb9519bc0c5c396f1-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1975.0,354.0 L1983.0,342.0 1967.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6d3fcfbb0d0945feb9519bc0c5c396f1-0-11\" stroke-width=\"2px\" d=\"M2170,352.0 C2170,264.5 2310.0,264.5 2310.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6d3fcfbb0d0945feb9519bc0c5c396f1-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2170,354.0 L2162,342.0 2178,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6d3fcfbb0d0945feb9519bc0c5c396f1-0-12\" stroke-width=\"2px\" d=\"M1995,352.0 C1995,177.0 2315.0,177.0 2315.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6d3fcfbb0d0945feb9519bc0c5c396f1-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2315.0,354.0 L2323.0,342.0 2307.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6d3fcfbb0d0945feb9519bc0c5c396f1-0-13\" stroke-width=\"2px\" d=\"M2345,352.0 C2345,264.5 2485.0,264.5 2485.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6d3fcfbb0d0945feb9519bc0c5c396f1-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2485.0,354.0 L2493.0,342.0 2477.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6d3fcfbb0d0945feb9519bc0c5c396f1-0-14\" stroke-width=\"2px\" d=\"M2695,352.0 C2695,177.0 3015.0,177.0 3015.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6d3fcfbb0d0945feb9519bc0c5c396f1-0-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">quantmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2695,354.0 L2687,342.0 2703,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6d3fcfbb0d0945feb9519bc0c5c396f1-0-15\" stroke-width=\"2px\" d=\"M2870,352.0 C2870,264.5 3010.0,264.5 3010.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6d3fcfbb0d0945feb9519bc0c5c396f1-0-15\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2870,354.0 L2862,342.0 2878,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6d3fcfbb0d0945feb9519bc0c5c396f1-0-16\" stroke-width=\"2px\" d=\"M2520,352.0 C2520,89.5 3020.0,89.5 3020.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6d3fcfbb0d0945feb9519bc0c5c396f1-0-16\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3020.0,354.0 L3028.0,342.0 3012.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'dep' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n",
      "Shutting down server on port 5000.\n"
     ]
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.serve(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Subword Tokenization  \n",
    "\n",
    "Tokenization is the process of breaking down a sentence into smaller units, enabling AI models to process text as discrete tokens rather than as a continuous block of text. In previous sections, you have used spaCy for tokenization, which primarily segments text into individual words. While this approach is efficient, it struggles with handling uncommon or out-of-vocabulary (OOV) words.  \n",
    "\n",
    "To address this limitation, modern tokenization techniques predominantly use **subword-based methods**. Instead of strictly segmenting text into words, these approaches break words into smaller subword units when necessary. For example, the word *unhappiness* might be tokenized into *un* and *happiness*. This strategy offers several advantages:  \n",
    "\n",
    "- **Improved Handling of Rare Words** – By decomposing words into meaningful subunits, the model can recognize and generate words that were not explicitly seen during training.  \n",
    "- **Compact Vocabulary** – Instead of storing an extensive vocabulary of all possible words, subword tokenization relies on a smaller set of subunits, which can be combined to form complex words.  \n",
    "- **Efficient Representation** – By balancing whole-word tokens with subword segments, this method optimizes both memory usage and model performance.  \n",
    "\n",
    "(**Note**: Often tokenizers try to maintain words that are frequently used, and split rare words into smaller subwords)\n",
    "\n",
    "We will therefore explore three subword tokenization techniques:  \n",
    "\n",
    "1. **WordPiece**  \n",
    "2. **Byte-Pair Encoding (BPE)**  \n",
    "3. **SentencePiece**  \n",
    "\n",
    "These tokenization methods have become standard in modern NLP models and are widely used in recent Large Language Models (LLMs).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, let's define a simple string that we will be tokenizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Natural Language Processing is incontrovertibly a good module.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. WordPiece Tokenization\n",
    "\n",
    "**WordPiece Tokenization** is a subword tokenization technique used in models like BERT (Bidirectional Encoder Representations from Transformers). It breaks down words into subwords, that can efficiently handle complex words, unknown terms, or out-of-vocabulary (OOV) words.\n",
    "\n",
    "WordPiece works by iteratively merging the most frequent pairs of characters or subword units in a large corpus. The resulting subwords represent the language's most frequent word components, which helps to reduce the size of the vocabulary while maintaining full language coverage.\n",
    "\n",
    "### Example: Tokenizing Text with BERT’s WordPiece Tokenizer\n",
    "\n",
    "In this section, we will use the Hugging Face `transformers` library to showcase how the BERT tokenizer works. We’ll tokenize a sample sentence, convert the tokens into token IDs, and then decode those IDs back into a human-readable string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/jl02958/miniconda3/envs/hamer_debug/lib/python3.10/site-packages (4.48.2)\n",
      "Requirement already satisfied: filelock in /home/jl02958/miniconda3/envs/hamer_debug/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /home/jl02958/miniconda3/envs/hamer_debug/lib/python3.10/site-packages (from transformers) (0.26.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jl02958/miniconda3/envs/hamer_debug/lib/python3.10/site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jl02958/miniconda3/envs/hamer_debug/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jl02958/miniconda3/envs/hamer_debug/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/jl02958/miniconda3/envs/hamer_debug/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/jl02958/miniconda3/envs/hamer_debug/lib/python3.10/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/jl02958/miniconda3/envs/hamer_debug/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/jl02958/miniconda3/envs/hamer_debug/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/jl02958/miniconda3/envs/hamer_debug/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/jl02958/miniconda3/envs/hamer_debug/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jl02958/miniconda3/envs/hamer_debug/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/jl02958/miniconda3/envs/hamer_debug/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jl02958/miniconda3/envs/hamer_debug/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/jl02958/miniconda3/envs/hamer_debug/lib/python3.10/site-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jl02958/miniconda3/envs/hamer_debug/lib/python3.10/site-packages (from requests->transformers) (2022.12.7)\n"
     ]
    }
   ],
   "source": [
    "# First, install the required library\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BERT Tokens: ['natural', 'language', 'processing', 'is', 'inc', '##ont', '##rove', '##rti', '##bly', 'a', 'good', 'module', '.']\n",
      "\n",
      "BERT Token IDs: [3019, 2653, 6364, 2003, 4297, 12162, 17597, 28228, 6321, 1037, 2204, 11336, 1012]\n",
      "\n",
      "Decoded Text: natural language processing is incontrovertibly a good module.\n"
     ]
    }
   ],
   "source": [
    "# Importing the necessary module from Hugging Face transformers\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Step 1: Load the pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Step 2: Tokenize the text into subword tokens\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"\\nBERT Tokens:\", tokens)\n",
    "\n",
    "# Step 3: Convert tokens to their corresponding token IDs\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"\\nBERT Token IDs:\", token_ids)\n",
    "\n",
    "# Step 4: Decode the token IDs back to human-readable text\n",
    "decoded_text = tokenizer.decode(token_ids)\n",
    "print(\"\\nDecoded Text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, the word **\"incontrovertibly\"**, which is quite rare, is split into **5 subwords** by the tokenizer. This process of splitting words into smaller subunits is particularly useful for handling rare or out-of-vocabulary (OOV) words.\n",
    "\n",
    "Each subword is represented as a **token**, and you can see that certain tokens are prefixed with `##`. This notation indicates that these subwords are continuations of a previous subword (i.e., they are not starting a new token). The tokenizer has broken down the word into smaller, more frequent subwords that are part of the model’s vocabulary.\n",
    "\n",
    "### Why Do We Use Token IDs?\n",
    "\n",
    "As shown above, the tokens are also associated with **token IDs**. These token IDs are numerical representations of the words or subwords. In the context of machine learning and NLP models, it's crucial to convert words into numbers because models operate on numerical data.\n",
    "\n",
    "Each token is mapped to a unique ID in the model’s vocabulary, which allows the model to process text efficiently. This conversion is essential because:\n",
    "\n",
    "- **Models can't understand raw text**: Machine learning models, including NLP models, don't process text directly. Instead, they process **numerical representations** of words.\n",
    "- **Token IDs map to model parameters**: The model's vocabulary is essentially a map of tokens (words or subwords) to unique IDs. These IDs are used by the model to look up the corresponding word embeddings (vector representations) in the model’s parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Byte-Pair Encoding (BPE) Tokenization\n",
    "\n",
    "**Byte-Pair Encoding (BPE)** is another popular subword tokenization technique used in models like GPT (Generative Pretrained Transformer) and other transformer-based architectures. \n",
    "\n",
    "### Byte-Level BPE Tokenization\n",
    "\n",
    "Instead of treating text as sequences of **Unicode characters** (such as 'a', 'b', 'c', etc.), **byte-level BPE** tokenizes text at the **byte level**. Each character, word, and symbol is first converted into its corresponding **byte representation**.\n",
    "\n",
    "The **base vocabulary** for byte-level BPE is much smaller, consisting of only **256 byte values**, as there are 256 possible byte values. This ensures that any character can be represented without needing to resort to an **unknown token** for out-of-vocabulary (OOV) words.\n",
    "\n",
    "This approach allows models like **GPT-2** and **RoBERTa** to handle any character or symbol, including those from different languages, special symbols, or rare characters, without needing additional vocabularies or dealing with OOV issues.\n",
    "\n",
    "\n",
    "### Example: Tokenizing Text with BPE Tokenizer\n",
    "\n",
    "In this section, we will use the Hugging Face `transformers` library to demonstrate how a BPE tokenizer works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BPE Tokens: ['Natural', 'ĠLanguage', 'ĠProcessing', 'Ġis', 'Ġinc', 'ont', 'ro', 'vert', 'ibly', 'Ġa', 'Ġgood', 'Ġmodule', '.']\n",
      "\n",
      "BPE Token IDs: [35364, 15417, 28403, 318, 753, 756, 305, 1851, 3193, 257, 922, 8265, 13]\n",
      "\n",
      "Decoded Text: Natural Language Processing is incontrovertibly a good module.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Step 1: Load the pre-trained BPE tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Step 2: Tokenize the text into subword tokens\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"\\nBPE Tokens:\", tokens)\n",
    "\n",
    "# Step 3: Convert tokens to their corresponding token IDs\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"\\nBPE Token IDs:\", token_ids)\n",
    "\n",
    "# Step 4: Decode the token IDs back to human-readable text\n",
    "decoded_text = tokenizer.decode(token_ids)\n",
    "print(\"\\nDecoded Text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Ġ Character in Byte-Pair Encoding (BPE)\n",
    "\n",
    "The output above reveals a noticeable difference: the **Ġ** character. In **byte-level BPE**, this character is used to indicate that a word token is preceded by a **space**. This is a crucial part of the tokenization strategy, as it helps BPE models distinguish between different words and their **contexts**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. SentencePiece Tokenization\n",
    "\n",
    "**SentencePiece** is also another popular subword tokenization technique used in models like T5 (Text-to-Text Transfer Transformer) and other transformer-based architectures.\n",
    "\n",
    "In this section, we will use the Hugging Face `transformers` library to demonstrate how the **SentencePiece tokenizer** works. However, we will also be training our own SentencePiece tokenizer afterwards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SentencePiece Tokens: ['▁Natural', '▁Language', '▁Processing', '▁is', '▁in', 'contro', 'vert', 'ibly', '▁', 'a', '▁good', '▁module', '.']\n",
      "\n",
      "SentencePiece Token IDs: [6869, 10509, 19125, 19, 16, 23862, 3027, 15596, 3, 9, 207, 6008, 5]\n",
      "\n",
      "Decoded Text: Natural Language Processing is incontrovertibly a good module.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "# Step 1: Load a pre-trained SentencePiece tokenizer (T5 model)\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Step 2: Tokenize the text into subword tokens using SentencePiece\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"\\nSentencePiece Tokens:\", tokens)\n",
    "\n",
    "# Step 3: Convert tokens to their corresponding token IDs\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"\\nSentencePiece Token IDs:\", token_ids)\n",
    "\n",
    "# Step 4: Decode the token IDs back to human-readable text\n",
    "decoded_text = tokenizer.decode(token_ids)\n",
    "print(\"\\nDecoded Text:\", decoded_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a SentencePiece Model\n",
    "\n",
    "In this section, we'll walk through how to **train a SentencePiece model** from a text corpus using **Byte-Pair Encoding (BPE)**. As seen from above, SentencePiece is a subword tokenization technique that efficiently handles rare or out-of-vocabulary (OOV) words by splitting them into smaller, manageable units.\n",
    "\n",
    "### Training Process Overview:\n",
    "1. **Input Corpus**: We use a text file (e.g., **Shakespeare_1_10.txt**) as input.\n",
    "2. **Model Parameters**:\n",
    "   - **Vocabulary size**: Set to **2000**.\n",
    "   - **Model type**: We use **BPE**.\n",
    "3. **Training**: The model is trained using `SentencePieceTrainer.train()` to learn subword units.\n",
    "4. **Output**: The model and vocabulary files are saved with the specified prefix (e.g., `mymodel.model`, `mymodel.vocab`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /home/jl02958/miniconda3/envs/hamer_debug/lib/python3.10/site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained and saved with prefix: mymodel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: Shakespear_1_10.txt\n",
      "  input_format: text\n",
      "  model_prefix: mymodel\n",
      "  model_type: BPE\n",
      "  vocab_size: 2000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: Shakespear_1_10.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 151 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=6143\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9512% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=65\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999512\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 151 sentences.\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 151\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 607\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=198 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=37 size=20 all=662 active=596 piece=▁f\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=24 size=40 all=839 active=773 piece=ee\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=60 all=982 active=916 piece=▁thee\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=80 all=1085 active=1019 piece=self\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=100 all=1143 active=1077 piece=ay\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=9 min_freq=0\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=120 all=1227 active=1078 piece=mb\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=140 all=1276 active=1127 piece=▁ti\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=160 all=1326 active=1177 piece=▁The\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=180 all=1349 active=1200 piece=▁sing\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=200 all=1384 active=1235 piece=nds\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=0\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=220 all=1401 active=1016 piece=▁why\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=240 all=1420 active=1035 piece=su\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=260 all=1450 active=1065 piece=ore\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=280 all=1460 active=1075 piece=▁eye\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=300 all=1455 active=1070 piece=▁being\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=0\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=320 all=1462 active=1008 piece=nt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=340 all=1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "486 active=1032 piece=ell\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=360 all=1506 active=1052 piece=▁In\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=380 all=1516 active=1062 piece=king\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=400 all=1524 active=1070 piece=▁giv\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=0\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=420 all=1522 active=997 piece=▁Look\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=440 all=1509 active=984 piece=▁lies\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=460 all=1500 active=975 piece=▁fresh\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=480 all=1481 active=956 piece=▁gentle\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=500 all=1465 active=940 piece=,’\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=0\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=520 all=1476 active=1012 piece=of\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=540 all=1472 active=1008 piece=▁7\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=560 all=1473 active=1009 piece=cor\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=580 all=1484 active=1020 piece=ife\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=600 all=1493 active=1029 piece=pro\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=0\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=620 all=1504 active=1010 piece=▁10\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=640 all=1498 active=1004 piece=▁ty\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=660 all=1500 active=1006 piece=ffic\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=680 all=1501 active=1007 piece=ming\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=700 all=1502 active=1008 piece=roys\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=0\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=720 all=1496 active=994 piece=▁Ser\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=740 all=1484 active=982 piece=▁foe\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=760 all=1473 active=971 piece=▁vie\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=780 all=1469 active=967 piece=rance\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=800 all=1461 active=959 piece=▁Upon\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=0\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=820 all=1447 active=987 piece=▁gaze\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=840 all=1428 active=968 piece=▁must\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=860 all=1410 active=950 piece=▁till\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=880 all=1397 active=937 piece=avenly\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=900 all=1394 active=934 piece=snowed\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=0\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=920 all=1377 active=984 piece=▁breed\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=940 all=1357 active=964 piece=▁grave\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=960 all=1340 active=947 piece=▁shall\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=980 all=1324 active=931 piece=thrifty\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=1000 all=1305 active=912 piece=▁feeble\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=0\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=1020 all=1285 active=981 piece=▁sounds\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=1040 all=1269 active=965 piece=▁Proving\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=1060 all=1249 active=945 piece=▁duteous\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=1080 all=1229 active=925 piece=▁tillage\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=1100 all=1209 active=905 piece=▁highmost\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=0\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=1120 all=1189 active=981 piece=▁forbidden\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=1140 all=1169 active=961 piece=▁distillation\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=1160 all=1149 active=941 piece=Ma\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=1180 all=1129 active=921 piece=Up\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=1200 all=1109 active=901 piece=cl\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=0 min_freq=0\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=1220 all=1089 active=981 piece=dy\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=1240 all=1069 active=961 piece=fu\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=1260 all=1049 active=941 piece=iq\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=1280 all=1029 active=921 piece=ni\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=1300 all=1009 active=901 piece=po\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=0 min_freq=0\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=1320 all=989 active=923 piece=ss\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=1340 all=969 active=903 piece=we\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=1360 all=949 active=883 piece=Die\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=1380 all=929 active=863 piece=Whe\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=1400 all=909 active=843 piece=bid\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=0 min_freq=0\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=1420 all=889 active=823 piece=ece\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=1440 all=869 active=803 piece=for\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=1460 all=849 active=783 piece=his\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=1480 all=829 active=763 piece=iou\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=1500 all=809 active=743 piece=lle\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=0 min_freq=0\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=1520 all=789 active=723 piece=non\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=1540 all=769 active=703 piece=orb\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=1560 all=749 active=683 piece=ppi\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=1580 all=729 active=663 piece=rip\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=1600 all=709 active=643 piece=sti\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=0 min_freq=0\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=1620 all=689 active=623 piece=utt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=1640 all=669 active=603 piece=▁At\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=1660 all=649 active=583 piece=▁cl\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=1680 all=629 active=563 piece=▁mi\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=1700 all=609 active=543 piece=▁wo\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=0 min_freq=0\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=1720 all=589 active=523 piece=dore\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=1740 all=569 active=503 piece=iege\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=1760 all=549 active=483 piece=ldst\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=1780 all=529 active=463 piece=oing\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=1800 all=509 active=443 piece=ried\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=0 min_freq=0\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=1820 all=489 active=423 piece=uest\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=1840 all=469 active=403 piece=▁Shi\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=1860 all=449 active=383 piece=▁bre\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=1880 all=429 active=363 piece=▁fue\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=1900 all=409 active=343 piece=▁liv\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=0 min_freq=0\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=1920 all=389 active=323 piece=▁orn\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: mymodel.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: mymodel.vocab\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Step 1: Define the input corpus file (a large text file)\n",
    "corpus_file = 'Shakespear_1_10.txt'  \n",
    "\n",
    "# Step 2: Define the model output directory and parameters\n",
    "model_prefix = 'mymodel' \n",
    "vocab_size = 2000\n",
    "model_type = 'bpe'  # BPE model (could also be 'unigram', 'char', etc.)\n",
    "\n",
    "# Step 3: Train the SentencePiece model\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=corpus_file,  \n",
    "    model_prefix=model_prefix,  \n",
    "    vocab_size=vocab_size,  \n",
    "    model_type=model_type, \n",
    "    character_coverage=0.9995,  # Coverage for character set (default is 0.9995)\n",
    "    input_format='text'  # Format of input (usually plain text)\n",
    ")\n",
    "\n",
    "print(f\"Model trained and saved with prefix: {model_prefix}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentencePiece Tokenization and Detokenization\n",
    "\n",
    "Once you’ve trained your **SentencePiece** model, you can use it to tokenize and detokenize sentences. The process involves converting a sentence into subword units (tokens) and then reconstructing the sentence from those tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenized sentence: ['▁I', '▁ha', 've', '▁su', 'ccess', 'fu', 'll', 'y', '▁tra', 'in', 'ed', '▁a', '▁S', 'ent', 'en', 'ce', 'P', 'ie', 'ce', '▁mo', 'de', 'l', '.']\n",
      "\n",
      "Detokenized sentence: I have successfully trained a SentencePiece model.\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('mymodel.model')\n",
    "\n",
    "# Tokenize a sentence\n",
    "sentence = \"I have successfully trained a SentencePiece model.\"\n",
    "tokens = sp.encode(sentence, out_type=str)  # or out_type=int for token IDs\n",
    "print(f\"\\nTokenized sentence: {tokens}\")\n",
    "\n",
    "# Detokenize the sentence\n",
    "detokenized = sp.decode(tokens)\n",
    "print(f\"\\nDetokenized sentence: {detokenized}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hamer_debug",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
