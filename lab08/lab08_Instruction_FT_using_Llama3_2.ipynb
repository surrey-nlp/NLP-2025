{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv2VWIAvYFgg"
      },
      "source": [
        "# A practical introduction to LLM fine-tuning\n",
        "\n",
        "This notebook has been taken from [`this`](https://github.com/ashishpatel26/LLM-Finetuning/blob/main/2.Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.ipynb) GitHub repo and adapted for the new model and the dataset.\n",
        "\n",
        "![](https://archive.is/0iIXL/f587d66c7324054f5ae1e81d7a5736567e8c15c8.webp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<a target=\"_blank\" href=\"https://colab.research.google.com/github/surrey-nlp/NLP-2025/blob/main/lab08/lab08_Instruction_FT_using_Llama3_2.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import HTML, display\n",
        "colab_button = HTML(\n",
        "    '<a target=\"_blank\" href=\"https://colab.research.google.com/github/surrey-nlp/NLP-2025/blob/main/lab08/lab08_Instruction_FT_using_Llama3_2.ipynb\">'\n",
        "    '<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>'\n",
        ")\n",
        "display(colab_button)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IfEVza1YFeI"
      },
      "source": [
        "# Background on fine-tuning LLMs\n",
        "\n",
        "![](https://archive.is/0iIXL/5f30742c57ad532b4cda9f1b48790dbcc7d00a85.webp)\n",
        "\n",
        "**Summary:**\n",
        "\n",
        "1. **LLM Pretraining:**\n",
        "   - Large Language Models (LLMs) are pretrained on extensive text corpora.\n",
        "   - Llama 3.2 was pretrained on a dataset of 3 trillion tokens, compared to BERT's training on BookCorpus and Wikipedia.\n",
        "   - Pretraining is resource-intensive and time-consuming.\n",
        "\n",
        "2. **Auto-Regressive Prediction:**\n",
        "   - Llama 3.2, an auto-regressive model, predicts the next token in a sequence.\n",
        "   - Auto-regressive models lack usefulness in providing instructions, leading to the need for instruction tuning.\n",
        "\n",
        "3. **Fine-Tuning Techniques:**\n",
        "   - Instruction tuning uses two main fine-tuning techniques:\n",
        "     a. Supervised Fine-Tuning (SFT): Trained on instruction-response datasets, minimizing differences between generated and actual responses.\n",
        "     b. Reinforcement Learning from Human Feedback (RLHF): Trained to maximize rewards based on human evaluations.\n",
        "\n",
        "4. **RLHF vs. SFT:**\n",
        "   - RLHF captures complex human preferences but requires careful reward system design and consistent human feedback.\n",
        "   - Direct Preference Optimization (DPO) an alternative to RLHF.\n",
        "   - SFT can be highly effective when the model hasn't encountered specific data during pretraining.\n",
        "\n",
        "5. **Effective SFT Example:**\n",
        "   - [`LIMA`](https://arxiv.org/abs/2305.11206) paper showed improved performance of LLaMA v1 model over GPT-3 by fine-tuning on a small high-quality dataset.\n",
        "   - Data quality and model size (e.g., 65b parameters) are crucial for successful fine-tuning.\n",
        "\n",
        "6. **Importance of Prompt Templates:**\n",
        "   - Prompt templates structure inputs: system prompt, user prompt, additional inputs, and model answer.\n",
        "   - Llama 3.2's template example: *\\<s>[INST] <<SYS>> System prompt <</SYS>> User prompt [/INST] Model answer </s>*\n",
        "   - Different templates (e.g., Alpaca, Vicuna) have varying impacts.\n",
        "\n",
        "7. **Reformatting for Llama 3.2:**\n",
        "   - Converting instruction dataset to Llama 2's template is important.\n",
        "   - The [`main`](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) dataset has already been reformatted for this purpose using [`this`](https://colab.research.google.com/drive/1ktwneRByMnm14i5fosRO_1PFCR0CAKFJ?usp=sharing) notebook.\n",
        "\n",
        "(Note: LLMs = Large Language Models, SFT = Supervised Fine-Tuning, RLHF = Reinforcement Learning from Human Feedback, DPO = Direct Preference Optimization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1EkaV2IYvk1"
      },
      "source": [
        "**Fine-Tuning Llama 3.2 with VRAM Limitations and QLoRA:**\n",
        "\n",
        "In this section, the goal is to fine-tune a Llama 3.2 model with 1 billion parameters using a T4 GPU with 16 GB of VRAM. Given the VRAM limitations, traditional fine-tuning is not feasible, necessitating parameter-efficient fine-tuning (PEFT) techniques like LoRA or QLoRA. The chosen approach is QLoRA, which employs 4-bit precision to drastically reduce VRAM usage.\n",
        "\n",
        "The following steps will be executed:\n",
        "\n",
        "1. **Environment Setup:**\n",
        "   The task involves leveraging the Hugging Face ecosystem and several libraries: transformers, accelerate, peft, trl, and bitsandbytes.\n",
        "\n",
        "2. **Installation and Library Loading:**\n",
        "   The first step is to install and load the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9q9RX0QvH6Mz",
        "outputId": "8f14ae58-a448-4c31-a31d-668da17b4a43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun Mar 23 01:21:31 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayObDY19eVTA",
        "outputId": "a73b7165-5c0a-463d-c741-7a83fcaaba6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.49.0)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.11/dist-packages (0.16.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.29.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from trl) (3.4.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl) (13.9.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=3.0.0->trl) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (3.11.14)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (2.18.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl) (1.18.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate peft bitsandbytes transformers trl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "htVrPtG7YAXH"
      },
      "outputs": [],
      "source": [
        "# Import necessary packages for the fine-tuning process\n",
        "import os                          # Operating system functionalities\n",
        "import torch                       # PyTorch library for deep learning\n",
        "from datasets import load_dataset  # Loading datasets for training\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,          # AutoModel for language modeling tasks\n",
        "    AutoTokenizer,                # AutoTokenizer for tokenization\n",
        "    BitsAndBytesConfig,           # Configuration for BitsAndBytes\n",
        "    HfArgumentParser,             # Argument parser for Hugging Face models\n",
        "    TrainingArguments,            # Training arguments for model training\n",
        "    pipeline,                     # Creating pipelines for model inference\n",
        "    logging,                      # Logging information during training\n",
        ")\n",
        "from peft import LoraConfig, PeftModel  # Packages for parameter-efficient fine-tuning (PEFT)\n",
        "from trl import SFTTrainer         # SFTTrainer for supervised fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3dgFGKE_evF7"
      },
      "outputs": [],
      "source": [
        "# !pip install -q datasets\n",
        "# !huggingface-cli login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hg72MpnpeeXX"
      },
      "source": [
        "---\n",
        "* **Section 1:** Parameters to tune\n",
        "    * Load the llama-3.2-1b model and fine-tune it on the mindhunter23/guanaco-llama2-1k dataset.\n",
        "    * The dataset contains 1,000 samples.\n",
        "    * You can find more information about the dataset in [`here`](https://huggingface.co/datasets/timdettmers/openassistant-guanaco).\n",
        "    * Feel free to use a different dataset.\n",
        "* **Section 2:** QLoRA parameters\n",
        "    * QLoRA will use a rank of 64 with a scaling parameter of 16.\n",
        "    * The Llama 2 model will be loaded directly in 4-bit precision using the NF4 type.\n",
        "    * The model will be trained for one epoch.\n",
        "* **Section 3:** Other parameters\n",
        "    * To get more information about the other parameters, check the [`TrainingArguments`](https://archive.is/o/0iIXL/https://huggingface.co/docs/transformers/main_classes/trainer%23transformers.TrainingArguments), [`PeftModel`](https://archive.is/o/0iIXL/https://huggingface.co/docs/peft/package_reference/peft_model), and [`SFTTrainer`](https://archive.is/o/0iIXL/https://huggingface.co/docs/trl/main/en/sft_trainer) documentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_NAQWYv9eRnM"
      },
      "outputs": [],
      "source": [
        "# The model that you want to train from the Hugging Face hub\n",
        "model_name = \"unsloth/Llama-3.2-1B\"\n",
        "# model_name = \"meta-llama/Llama-3.2-1B\"\n",
        "# model_name = \"NousResearch/Llama-2-7b-hf\"\n",
        "\n",
        "# The instruction dataset to use\n",
        "dataset_name = \"mindhunter23/guanaco-llama2-1k-en\"\n",
        "\n",
        "# Fine-tuned model name\n",
        "new_model = \"llama-3.2-1b-miniguanaco\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BGufVOIOe23a"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# QLoRA parameters\n",
        "################################################################################\n",
        "\n",
        "# LoRA attention dimension\n",
        "lora_r = 64\n",
        "\n",
        "# Alpha parameter for LoRA scaling\n",
        "lora_alpha = 16\n",
        "\n",
        "# Dropout probability for LoRA layers\n",
        "lora_dropout = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YrAujYOee6SV"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# bitsandbytes parameters\n",
        "################################################################################\n",
        "\n",
        "# Activate 4-bit precision base model loading\n",
        "use_4bit = True\n",
        "\n",
        "# Compute dtype for 4-bit base models\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "\n",
        "# Quantization type (fp4 or nf4)\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "\n",
        "# Activate nested quantization for 4-bit base models (double quantization)\n",
        "use_nested_quant = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tDE0lx-Pe7ki"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# TrainingArguments parameters\n",
        "################################################################################\n",
        "\n",
        "# Output directory where the model predictions and checkpoints will be stored\n",
        "output_dir = \"./results\"\n",
        "\n",
        "# Number of training epochs\n",
        "num_train_epochs = 1\n",
        "\n",
        "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
        "fp16 = False\n",
        "bf16 = False\n",
        "\n",
        "# Batch size per GPU for training\n",
        "per_device_train_batch_size = 1\n",
        "\n",
        "# Batch size per GPU for evaluation\n",
        "per_device_eval_batch_size = 1\n",
        "\n",
        "# Number of update steps to accumulate the gradients for\n",
        "gradient_accumulation_steps = 4\n",
        "\n",
        "# Enable gradient checkpointing\n",
        "gradient_checkpointing = True\n",
        "\n",
        "# Maximum gradient normal (gradient clipping)\n",
        "max_grad_norm = 0.3\n",
        "\n",
        "# Initial learning rate (AdamW optimizer)\n",
        "learning_rate = 2e-4\n",
        "\n",
        "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
        "weight_decay = 0.001\n",
        "\n",
        "# Optimizer to use\n",
        "optim = \"paged_adamw_32bit\"\n",
        "\n",
        "# Learning rate schedule (constant a bit better than cosine)\n",
        "lr_scheduler_type = \"constant\"\n",
        "\n",
        "# Number of training steps (overrides num_train_epochs)\n",
        "max_steps = -1\n",
        "\n",
        "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
        "warmup_ratio = 0.03\n",
        "\n",
        "# Group sequences into batches with same length\n",
        "# Saves memory and speeds up training considerably\n",
        "group_by_length = True\n",
        "\n",
        "# Save checkpoint every X updates steps\n",
        "save_steps = 25\n",
        "\n",
        "# Log every X updates steps\n",
        "logging_steps = 25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BBXapfcZe-U8"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# SFT parameters\n",
        "################################################################################\n",
        "\n",
        "# Maximum sequence length to use\n",
        "max_seq_length = None\n",
        "\n",
        "# Pack multiple short examples in the same input sequence to increase efficiency\n",
        "packing = False\n",
        "\n",
        "# Load the entire model on the GPU 0\n",
        "device_map = {\"\": 0}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rxzdSG2fFX7"
      },
      "source": [
        "\n",
        "1. **Loading the Dataset:**\n",
        "   The first step involves loading the preprocessed dataset. This dataset will be used for fine-tuning. Preprocessing might involve reformatting prompts, filtering out low-quality text, and combining multiple datasets if needed.\n",
        "\n",
        "2. **Configuring BitsAndBytes for 4-bit Quantization:**\n",
        "   The `BitsAndBytesConfig` is set up to enable 4-bit quantization. This configuration is crucial for reducing the memory usage during fine-tuning.\n",
        "\n",
        "3. **Loading Llama 3.2 Model and Tokenizer in 4-bit Precision:**\n",
        "   The Llama 3.2 model is loaded with 4-bit precision, which significantly reduces the memory footprint. The corresponding tokenizer is also loaded to preprocess the text data.\n",
        "\n",
        "4. **Loading Configurations and Initializing SFTTrainer:**\n",
        "   - The configurations needed for QLoRA, which is a parameter-efficient fine-tuning technique, are loaded.\n",
        "   - Regular training parameters are set up.\n",
        "   - The `SFTTrainer` is initialized with all the loaded configurations and parameters. This trainer will manage the supervised fine-tuning process.\n",
        "\n",
        "5. **Start of Training:**\n",
        "   After all the necessary components are loaded and configured, the training process begins. The `SFTTrainer` takes care of fine-tuning the Llama 3.2 model using the specified dataset, configurations, and parameters.\n",
        "   \n",
        "  These steps collectively set up the environment for fine-tuning a Llama 3.2 model with 1 billion parameters in 4-bit precision using the QLoRA technique, thus optimizing for VRAM limitations while maintaining model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajqpAQQoe_vk",
        "outputId": "75b5e360-b46b-4a89-f609-d82a7544bc9e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Step 1 : Load dataset (you can process it here)\n",
        "dataset = load_dataset(dataset_name, split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_meFORX1fMgu"
      },
      "outputs": [],
      "source": [
        "# Step 2 : Load tokenizer and model with QLoRA configuration\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "\n",
        "# TODO: complete the function arguments using initializations done before\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=...,\n",
        "    bnb_4bit_quant_type=...,\n",
        "    bnb_4bit_compute_dtype=...,\n",
        "    bnb_4bit_use_double_quant=...,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "oeuDUNAlfOyP"
      },
      "outputs": [],
      "source": [
        "# Step 3 :Check GPU compatibility with bfloat16\n",
        "if compute_dtype == torch.float16 and use_4bit:\n",
        "    major, _ = torch.cuda.get_device_capability()\n",
        "    if major >= 8:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
        "        print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XQuXIZfffQQA"
      },
      "outputs": [],
      "source": [
        "# Step 4 :Load base model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "p4LEH6J2fRcv"
      },
      "outputs": [],
      "source": [
        "# Step 5 :Load LLaMA tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXTnIGwthB_p"
      },
      "source": [
        "## LoRA Configuration:\n",
        "\n",
        "Low-Rank Adaptation (LoRA) is a parameter-efficient fine-tuning technique that injects trainable rank decomposition matrices into each layer of the Transformer model, effectively increasing the expressiveness of the model with a small number of extra parameters.\n",
        "\n",
        "The following code configures the LoRA parameters for fine-tuning:\n",
        "\n",
        "* **`lora_alpha`:** This parameter controls the scaling factor for the LoRA updates. A higher value means the LoRA updates will have a larger impact on the model's weights.\n",
        "* **`lora_dropout`:** This parameter sets the dropout probability for the LoRA layers. Dropout is a regularization technique that helps prevent overfitting.\n",
        "* **`r`:** This parameter determines the rank of the LoRA decomposition matrices. A lower rank leads to fewer trainable parameters but might limit the expressiveness of the LoRA updates.\n",
        "* **`bias`:** This parameter specifies whether to apply a bias term to the LoRA updates. \"none\" means no bias is used.\n",
        "* **`task_type`:** This parameter indicates the type of task the model is being fine-tuned for. In this case, it's \"CAUSAL_LM\" for causal language modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "9JFXcBfNitQl"
      },
      "outputs": [],
      "source": [
        "# Step 6 :Load LoRA configuration\n",
        "\n",
        "# TODO: complete the function arguments for LoRA config\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=...,\n",
        "    lora_dropout=...,\n",
        "    r=...,\n",
        "    bias=...,\n",
        "    task_type=...,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "w9sTAqRUiukA"
      },
      "outputs": [],
      "source": [
        "# Step 7 :Set training parameters\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    report_to=\"tensorboard\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7ArmUGTiwAY"
      },
      "outputs": [],
      "source": [
        "# Step 8 :Set supervised fine-tuning parameters\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    # dataset_text_field=\"text\",\n",
        "    # max_seq_length=None,\n",
        "    # tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    # packing=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vksg2eIFixWj"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Step 9 :Train model\n",
        "trainer.train()\n",
        "\n",
        "# Step 10 :Save trained model\n",
        "trainer.model.save_pretrained(new_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvVEYAhQ_grc"
      },
      "source": [
        "You might notice constant variations in the training loss during the fine-tuning process. This is expected and can be attributed to factors such as:\n",
        "\n",
        "* **Small Model Size:** The Llama 3.2-1B model used here has a relatively small number of parameters compared to larger LLMs. This can lead to more sensitivity to individual training examples and fluctuations in the loss.\n",
        "* **Dataset Size and Variability:** The dataset used for fine-tuning is limited in size and contain a diverse range of prompts and responses. This variability can contribute to fluctuations in the loss as the model learns to generalize across different types of examples.\n",
        "* **Batch Size:** A small batch size, such as the one used here (per_device_train_batch_size = 1), can also contribute to variations in the loss. With a smaller batch size, the model updates its parameters more frequently based on a limited number of examples, which can lead to fluctuations in the loss curve.\n",
        "\n",
        "Despite these variations, the overall trend of the loss should generally be decreasing, indicating that the model is learning and improving over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGv60Uo0jR8s"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir results/runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xepQkADWLNe"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate rouge_score bert_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7pfgrQg-1y9"
      },
      "source": [
        "## Evaluating the Fine-tuned Model:\n",
        "\n",
        "In this section, we evaluate the fine-tuned model using ROUGE and BERTScore metrics. We will generate text from the model using the prompts in the test set and compare the generated text with the reference responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Y32XqsbaFwF"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def extract_prompt_response(text):\n",
        "    \"\"\"\n",
        "    Extracts the prompt and response from the given text using regex.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text containing the prompt and response.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the prompt and response, or None if not found.\n",
        "    \"\"\"\n",
        "    match = re.search(r\"<s>\\[INST\\](.*?)\\[/INST\\](.*?)</s>\", text, re.DOTALL)\n",
        "    if match:\n",
        "        return match.group(1).strip(), match.group(2).strip()\n",
        "    else:\n",
        "        return None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YE7eay_GJX4y"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "eval_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=...   # TODO: include quantization config\n",
        "    device_map=device_map\n",
        ")\n",
        "eval_model = PeftModel.from_pretrained(eval_model, new_model)  # Load LoRA weights\n",
        "eval_tokenizer = ...  # TODO: use the otkenizer from the trained model\n",
        "eval_model.eval()\n",
        "\n",
        "# Prepare the test split\n",
        "test_dataset = load_dataset(dataset_name, split=\"test\")\n",
        "\n",
        "# Perform inference and evaluate using ROUGE-L and BERTScore\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "bertscore = evaluate.load(\"bertscore\")\n",
        "\n",
        "predictions = []\n",
        "references = []\n",
        "\n",
        "for i in range(min(100, len(test_dataset))): # perform inference on 100 samples\n",
        "    example = test_dataset[i]\n",
        "    prompt, response = extract_prompt_response(example[\"text\"])\n",
        "\n",
        "    if prompt and response:\n",
        "        inputs = eval_tokenizer(prompt, return_tensors=\"pt\").to(eval_model.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = eval_model.generate(**inputs, max_new_tokens=50) # Adjust max_new_tokens as needed\n",
        "        generated_text = eval_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        predictions.append(generated_text)\n",
        "        references.append(response)\n",
        "    else:\n",
        "        print(f\"Skipping example: {example['text'][:50]}... (Invalid format)\")\n",
        "\n",
        "# Calculate ROUGE-L scores\n",
        "rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
        "\n",
        "# Calculate BERTScore scores\n",
        "bertscore_scores = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
        "\n",
        "print(\"ROUGE-L scores:\", rouge_scores)\n",
        "# print(\"BERTScore scores:\", bertscore_scores)\n",
        "\n",
        "average_bertscore = np.mean(bertscore_scores[\"f1\"])\n",
        "\n",
        "print(\"Average BERTScore:\", average_bertscore)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3CulkvD90yZ"
      },
      "source": [
        "## Explanation of ROUGE Scores:\n",
        "\n",
        "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a set of metrics used to evaluate automatic summarization of texts.\n",
        "The ROUGE-L scores you see above represent the following:\n",
        "\n",
        "* **rouge1:** Measures the overlap of unigrams (single words) between the generated text and the reference text.\n",
        "* **rouge2:** Measures the overlap of bigrams (two-word sequences) between the generated text and the reference text.\n",
        "* **rougeL:**  Measures the longest common subsequence (LCS) between the generated text and the reference text. It is generally considered a better measure of recall than ROUGE-1 and ROUGE-2, as it considers sentence-level structure.\n",
        "* **rougeLsum:** Measures the longest common subsequence (LCS) between the generated text and the reference text, but it is calculated over the summary level rather than sentence level. This metric was introduced in a later version of ROUGE.\n",
        "\n",
        "Higher scores for all these metrics indicate better performance, meaning the generated text is more similar to the reference text.\n",
        "\n",
        "## Explanation of BERTScore:\n",
        "\n",
        "BERTScore is an automatic evaluation metric that measures the similarity between two texts using contextual embeddings from BERT. It computes a similarity score based on the cosine similarity between the token embeddings of the generated text and the reference text.\n",
        "\n",
        "The average BERTScore you see above represents the average F1 score over the entire test set. The F1 score is the harmonic mean of precision and recall, which provides a balanced measure of performance.\n",
        "\n",
        "Higher BERTScore indicates better performance, meaning the generated text is semantically more similar to the reference text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgYgknKJ94y1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp24",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
