{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Lab 7b: Text generation with GPT"
      ],
      "metadata": {
        "id": "G37uSMRtq5Qc"
      },
      "id": "G37uSMRtq5Qc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the second part of this lab, we will experiment with loading a GPT-2 model for the same task. We will also utilize the `tiny_shakespeare` dataset and all of the metrics in the first part to evaluate the model\n"
      ],
      "metadata": {
        "id": "1yiqnnPsYV-L"
      },
      "id": "1yiqnnPsYV-L"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8b9738a4-3d6a-462b-8fc0-8524f3e2ce76",
      "metadata": {
        "id": "8b9738a4-3d6a-462b-8fc0-8524f3e2ce76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb14a348-ce8c-4c0d-d458-ef2456f57b8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.4.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Collecting bert_score\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting torcheval\n",
            "  Downloading torcheval-0.0.7-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert_score) (3.10.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (3.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.4.0-py3-none-any.whl (487 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.4/487.4 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torcheval-0.0.7-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.2/179.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, torcheval, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, dill, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets, bert_score\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bert_score-0.3.13 datasets-3.4.0 dill-0.3.8 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 tiktoken-0.9.0 torcheval-0.0.7 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch numpy transformers datasets tiktoken tqdm nltk bert_score torcheval"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d6bjUwkU66G",
        "outputId": "fe823aac-6c9f-464a-be4d-d501763bff25"
      },
      "id": "1d6bjUwkU66G",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Mar 16 21:12:11 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a61cd73b-213d-453a-a296-46232cf0b8ec",
      "metadata": {
        "id": "a61cd73b-213d-453a-a296-46232cf0b8ec"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import tiktoken\n",
        "import numpy as np\n",
        "import pickle\n",
        "import torch\n",
        "import time\n",
        "import math\n",
        "from contextlib import nullcontext\n",
        "\n",
        "# *** don't forget to upload model.py into /content ***\n",
        "from model import GPTConfig, GPT"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52faadf3-03a3-4684-bdfc-f0d589e86cc7",
      "metadata": {
        "id": "52faadf3-03a3-4684-bdfc-f0d589e86cc7"
      },
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first download the `tiny_shakespeare` dataset with the following:"
      ],
      "metadata": {
        "id": "a_jl1anh7yq3"
      },
      "id": "a_jl1anh7yq3"
    },
    {
      "cell_type": "code",
      "source": [
        "input_file_path = os.path.join(os.path.abspath(''), 'input.txt')\n",
        "if not os.path.exists(input_file_path):\n",
        "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "    with open(input_file_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(requests.get(data_url).text)\n",
        "\n",
        "with open(input_file_path, 'r', encoding='utf-8') as f:\n",
        "    data = f.read()\n",
        "n = len(data)\n",
        "train_data = data[:int(n*0.9)]\n",
        "val_data = data[int(n*0.9):]\n",
        "\n",
        "# encode with tiktoken gpt2 bpe\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "train_ids = enc.encode_ordinary(train_data)\n",
        "val_ids = enc.encode_ordinary(val_data)\n",
        "print(f\"train has {len(train_ids):,} tokens\")\n",
        "print(f\"val has {len(val_ids):,} tokens\")\n",
        "\n",
        "# export to bin files\n",
        "train_ids = np.array(train_ids, dtype=np.uint16)\n",
        "val_ids = np.array(val_ids, dtype=np.uint16)\n",
        "train_ids.tofile(os.path.join(os.path.abspath(''), 'train.bin'))\n",
        "val_ids.tofile(os.path.join(os.path.abspath(''), 'val.bin'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8FZY_5Abirw",
        "outputId": "bfefa7b8-48c6-413a-d4fe-9ebf8643467e"
      },
      "id": "C8FZY_5Abirw",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train has 301,966 tokens\n",
            "val has 36,059 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we will define some variables and function for the subsequent code to work."
      ],
      "metadata": {
        "id": "AIDd9Sv39Vsv"
      },
      "id": "AIDd9Sv39Vsv"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a5a63e56-9ce0-4738-a800-f907b93168a3",
      "metadata": {
        "id": "a5a63e56-9ce0-4738-a800-f907b93168a3"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
        "\n",
        "seed = 1337\n",
        "dtype = 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
        "compile = True # use PyTorch 2.0 to compile the model to be faster\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "\n",
        "def get_batch(split, batch_size=16, block_size=1024):\n",
        "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
        "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
        "    if split == 'train':\n",
        "        data = np.memmap(os.path.join(os.path.abspath(''), 'train.bin'),\n",
        "                         dtype=np.uint16, mode='r')\n",
        "    else:\n",
        "        data = np.memmap(os.path.join(os.path.abspath(''), 'val.bin'),\n",
        "                         dtype=np.uint16, mode='r')\n",
        "\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i + block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i + 1:i + 1 + block_size]).astype(np.int64)) for i in ix])\n",
        "    if device_type == 'cuda':\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "23b13972-f456-49e4-a59a-afa2d4fb721c",
      "metadata": {
        "id": "23b13972-f456-49e4-a59a-afa2d4fb721c"
      },
      "outputs": [],
      "source": [
        "# assume gpt-2 encodings by default\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
        "decode = lambda l: enc.decode(l)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4384534b-012f-42a9-9a50-7dc55944754c",
      "metadata": {
        "id": "4384534b-012f-42a9-9a50-7dc55944754c"
      },
      "source": [
        "---\n",
        "Now let's load a pre-trained GPT-2 model and see, how does it perform in terms of the calculated metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "06326618-1de3-45af-bb19-2cf327fe7755",
      "metadata": {
        "id": "06326618-1de3-45af-bb19-2cf327fe7755",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a37b4fb-3872-46e9-d16d-ffa27039e1e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading weights from pretrained gpt: gpt2-medium\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 353.77M\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "init_from = 'gpt2-medium'  # 'gpt2-xl' if you have access to a decent GPU\n",
        "\n",
        "# init from a given GPT-2 model\n",
        "model = GPT.from_pretrained(init_from, dict(dropout=0.0))\n",
        "model.eval()\n",
        "model.to(device)\n",
        "if compile:\n",
        "    model = torch.compile(model) # requires PyTorch 2.0 (optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3fc6e0f-1919-4740-b7c0-cfe32c0f2d69",
      "metadata": {
        "id": "f3fc6e0f-1919-4740-b7c0-cfe32c0f2d69"
      },
      "source": [
        "Sampling the model with a given context, observe the key differences between our previous trained model, and this current one which has been trained with an alternative set of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "00abae0d-8e69-4fd7-9662-c15df300ee29",
      "metadata": {
        "id": "00abae0d-8e69-4fd7-9662-c15df300ee29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1735bf9-a08f-43a0-c730-fc742a951c3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Universe is vast. The Universe is infinite. And the Universe is not so much a place that we are in as it is a place that has been created under the guidance of God.\n",
            "\n",
            "So, what do you do with that?\n",
            "\n",
            "We call it the Creator-Conserver relationship.\n",
            "\n",
            "That is, we meet God to worship Him, and our true God is God Himself. And it is actually very difficult to find a place in this world where you cannot find a believer who does not worship God. Most of our friends and neighbors do not worship God. They worship others more readily. But we do.\n",
            "\n",
            "But what does\n",
            "==============================\n",
            "The Universe is vast, but we are only one of millions.\"\n",
            "\n",
            "This article appeared in print under the headline \"The cosmos, 1.8 billion years since the Big Bang\"<|endoftext|>Perez is part of a growing cohort of players, including Atlanta United's forward Alvaro Saborio and the University of Arizona's Alex Crognale, who are seeing their careers take a slight turn for the worse.\n",
            "\n",
            "Part of the problem is the loss of depth as the league's top few teams have moved for younger talents, which is compounded by more clubs having difficult-to-fill holes during the transfer window.\n",
            "\n",
            "That's\n",
            "==============================\n",
            "The Universe is vast\n",
            "\n",
            "Each of the seven vast worlds is nearly as vast as the entire Milky Way galaxy.\n",
            "\n",
            "But it's the Universe that's really remarkable.\n",
            "\n",
            "On average, planets in the Universe are estimated to be about 100 to 150 times the mass of Earth and 0.5 to 1.6 times the diameter of Manhattan.\n",
            "\n",
            "The largest object in the Universe is the massive black hole at the centre of a supermassive black hole.\n",
            "\n",
            "(Image: NASA/JPL)\n",
            "\n",
            "It's estimated that around 370 trillion stars are embedded within the largest known supermassive black holes.\n",
            "\n",
            "All of these stars,\n",
            "==============================\n",
            "The Universe is vast and it is mysterious. But we have enough knowledge to see it. We can predict the weather through our watches and thermometers. And we can make educated guesses about why certain events tend to happen, even while others don't. Perhaps it's a natural phenomenon or human creativity. Perhaps it's a certain relationship between stars and planets. Or maybe it's just that we look at pretty much everything in our universe and think we know everything.\n",
            "\n",
            "Think about our entire universe. Even just the smallest things, whether it's a molecule or a star, they appear as something very remote and insignificant; more abstract and subnational than they are\n",
            "==============================\n",
            "The Universe is vast, but there may be only six Earth-sized planets, according to the new study.\n",
            "\n",
            "\"The small number of Earth-sized planets around other stars that we know about makes it hard to make predictions about the evolution of these planets,\" study co-author and astronomer Tod M. Bailey of the University of California, Berkeley, said in a statement.\n",
            "\n",
            "Advertisement\n",
            "\n",
            "The plan to find and study larger planets is a major step toward revealing the structure and composition of the Universe, said study co-author and astronomy professor Kairi M. Wilson of the University of Hawaii-Manoa in Kapolei.\n",
            "\n",
            "==============================\n"
          ]
        }
      ],
      "source": [
        "# encode the beginning of the prompt\n",
        "context = 'The Universe is vast'\n",
        "start_ids = encode(context)\n",
        "num_samples = 5\n",
        "sample_len = 128\n",
        "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        "top_k = 200\n",
        "\n",
        "with torch.no_grad():\n",
        "    with ctx:\n",
        "        for k in range(num_samples):\n",
        "            x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
        "\n",
        "            probs, y = model.generate(x, sample_len, temperature=temperature, top_k=top_k)\n",
        "            print(decode(y[0].tolist()))\n",
        "            print('==============================')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "934e03b7-5206-4131-a6ac-82341c3e4f90",
      "metadata": {
        "id": "934e03b7-5206-4131-a6ac-82341c3e4f90"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\"><b>Challenge 4:</b> Re-use the code from above to run the model on the validation dataset and calculate BLEU score, BERTScore and perplexity. What do you observe? Do numbers correlate with the qualitative evaluation?</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "74a8736e-fcb4-48fb-b4f8-5a61425c904a",
      "metadata": {
        "id": "74a8736e-fcb4-48fb-b4f8-5a61425c904a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a3226e0-e37a-4460-dcd3-c9dc934d2c38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch 0/17\n",
            "batch 1/17\n",
            "batch 2/17\n",
            "batch 3/17\n",
            "batch 4/17\n",
            "batch 5/17\n",
            "batch 6/17\n",
            "batch 7/17\n",
            "batch 8/17\n",
            "batch 9/17\n",
            "batch 10/17\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# experiment with sample length, context size and their influence on the evaluation metrics\n",
        "sample_len = 64\n",
        "batch_size = 32\n",
        "start_len = 5\n",
        "\n",
        "temperature = 0.8\n",
        "top_k = 200\n",
        "# -------------------------------------\n",
        "\n",
        "val_data = np.memmap('./val.bin', dtype=np.uint16, mode='r')\n",
        "num_batches = len(val_data) // sample_len // batch_size\n",
        "\n",
        "pred_sent = []\n",
        "gt_sent = []\n",
        "\n",
        "pred_tokens = []\n",
        "gt_tokens = []\n",
        "\n",
        "pred_probs = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    with ctx:\n",
        "        for batch_i in range(num_batches):\n",
        "          print(f'batch {batch_i}/{num_batches}')\n",
        "\n",
        "          if batch_i == 10:\n",
        "            break\n",
        "\n",
        "          X_val, _ = get_batch('val', batch_size, sample_len)\n",
        "\n",
        "          for k in range(batch_size):\n",
        "              start_ids = X_val[k, :start_len]\n",
        "              x = start_ids.clone().detach().type(torch.long).to(device)[None, ...]\n",
        "\n",
        "              probs, pred = model.generate(x, sample_len-start_len, temperature=temperature, top_k=top_k)\n",
        "              pred_probs.append(torch.cat(probs).cpu())\n",
        "\n",
        "              # skip the \"context\" that was provided\n",
        "              decoded_pred = decode(pred[0, start_len:].tolist())\n",
        "              pred_sent.append(decoded_pred)\n",
        "\n",
        "              decoded_gt = decode(X_val[k, start_len:].tolist())\n",
        "              gt_sent.append(decoded_gt)\n",
        "\n",
        "              gt_tokens.append([X_val[k, start_len:].cpu().numpy()])\n",
        "              pred_tokens.append(pred[0, start_len:].cpu().numpy())\n",
        "\n",
        "pred_sent = pred_sent[:120]\n",
        "gt_sent = gt_sent[:120]\n",
        "gt_tokens = gt_tokens[:120]\n",
        "pred_tokens = pred_tokens[:120]\n",
        "pred_probs = pred_probs[:120]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "print('BLEU-1: ', corpus_bleu(gt_tokens, pred_tokens, weights=(1.0, 0, 0, 0)))\n",
        "print('BLEU-2: ', corpus_bleu(gt_tokens, pred_tokens, weights=(0, 1.0, 0, 0)))\n",
        "print('BLEU-3: ', corpus_bleu(gt_tokens, pred_tokens, weights=(0, 0, 1.0, 0)))\n",
        "print('BLEU-4: ', corpus_bleu(gt_tokens, pred_tokens, weights=(0, 0, 0, 1.0)))"
      ],
      "metadata": {
        "id": "ORkIb6rljltv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea7a99d3-e0e0-44ad-d625-2f11db839f8f"
      },
      "id": "ORkIb6rljltv",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU-1:  0.18022598870056494\n",
            "BLEU-2:  0.028448275862068963\n",
            "BLEU-3:  0.007602339181286548\n",
            "BLEU-4:  0.0005952380952380952\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bert_score import BERTScorer\n",
        "scorer = BERTScorer(model_type='bert-base-uncased')\n",
        "P, R, F1 = scorer.score(pred_sent, gt_sent)\n",
        "\n",
        "print(f\"BERTScore Precision: {P.mean():.4f}, Recall: {R.mean():.4f}, F1: {F1.mean():.4f}\")"
      ],
      "metadata": {
        "id": "Vn-9PHhJjtf5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06141a98-d9f3-4a8c-8711-90d88adfc409"
      },
      "id": "Vn-9PHhJjtf5",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERTScore Precision: 0.3928, Recall: 0.3860, F1: 0.3891\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torcheval.metrics.functional.text import perplexity\n",
        "# 3d tensor of token probabilities: (num_samples, num_tokens, vocab size)\n",
        "perp_probs = torch.tensor(np.array(pred_probs))\n",
        "print(perp_probs.size())\n",
        "\n",
        "# 2d tensor of gt tokens: (num_samples, num_tokens)\n",
        "perp_gt = torch.stack([torch.from_numpy(elem[0]) for elem in gt_tokens])\n",
        "print(perp_gt.size())\n",
        "\n",
        "print('Perplexity: ', perplexity(perp_probs, perp_gt).item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpRLVsNW6Ynk",
        "outputId": "661e4803-67e5-4271-df81-5b70509a8849"
      },
      "id": "tpRLVsNW6Ynk",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([120, 59, 50257])\n",
            "torch.Size([120, 59])\n",
            "Perplexity:  49529.6875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34b02909-e0cd-4f41-89a3-8876b9203efa",
      "metadata": {
        "id": "34b02909-e0cd-4f41-89a3-8876b9203efa"
      },
      "source": [
        "> Do you see drawbacks of the metrics that rely on the reference text? Can we provide an adequate reference in case of an unconstrained text generation? Compare the outputs of both models qualitatively. Think about the other ways to evaluate the text generation models."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}