{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "G37uSMRtq5Qc",
      "metadata": {
        "id": "G37uSMRtq5Qc"
      },
      "source": [
        "## Lab 7a: Text generation with GPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b4e9396d",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<a target=\"_blank\" href=\"https://colab.research.google.com/github/surrey-nlp/NLP-2025/blob/main/lab07/lab07a-Text_Generation_with_GPT.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import HTML, display\n",
        "colab_button = HTML(\n",
        "    '<a target=\"_blank\" href=\"https://colab.research.google.com/github/surrey-nlp/NLP-2025/blob/main/lab07/lab07a-Text_Generation_with_GPT.ipynb\">'\n",
        "    '<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>'\n",
        ")\n",
        "display(colab_button)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1yiqnnPsYV-L",
      "metadata": {
        "id": "1yiqnnPsYV-L"
      },
      "source": [
        "During this lab, we will further explore the transformer architecture and GPT. The GPT (Generative Pre-trained Transformer) architecture has significantly advanced the field of NLP by enabling the development of powerful and versatile language models. Its transformer-based design, coupled with unsupervised pre-training on large text corpora, has revolutionized tasks such as text generation, summarization, and language understanding.\n",
        "\n",
        "Even though we are not able to perform a large-scale training in the scope of this lab, we can still explore the capabilities of the model on a smaller scale by training on the `tiny_shakespeare` dataset and utilizing some pre-trained weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b9738a4-3d6a-462b-8fc0-8524f3e2ce76",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b9738a4-3d6a-462b-8fc0-8524f3e2ce76",
        "outputId": "73b836d4-8d75-4acf-ac60-a1094d745460"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.4.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Collecting bert_score\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting torcheval\n",
            "  Downloading torcheval-0.0.7-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert_score) (3.10.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (3.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.4.0-py3-none-any.whl (487 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.4/487.4 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torcheval-0.0.7-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.2/179.2 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, torcheval, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, dill, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets, bert_score\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bert_score-0.3.13 datasets-3.4.0 dill-0.3.8 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 tiktoken-0.9.0 torcheval-0.0.7 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch numpy transformers datasets tiktoken tqdm nltk bert_score torcheval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d6bjUwkU66G",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d6bjUwkU66G",
        "outputId": "fe823aac-6c9f-464a-be4d-d501763bff25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun Mar 16 21:12:11 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a61cd73b-213d-453a-a296-46232cf0b8ec",
      "metadata": {
        "id": "a61cd73b-213d-453a-a296-46232cf0b8ec"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import tiktoken\n",
        "import numpy as np\n",
        "import pickle\n",
        "import torch\n",
        "import time\n",
        "import math\n",
        "from contextlib import nullcontext\n",
        "\n",
        "# don't forget to upload model.py into /content\n",
        "from model import GPTConfig, GPT"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52faadf3-03a3-4684-bdfc-f0d589e86cc7",
      "metadata": {
        "id": "52faadf3-03a3-4684-bdfc-f0d589e86cc7"
      },
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1459705-e77d-47b7-b2e7-d8ffbd107bb9",
      "metadata": {
        "id": "c1459705-e77d-47b7-b2e7-d8ffbd107bb9"
      },
      "source": [
        "Download the `tiny_shakespeare` dataset, which consists of numerous Shakespeare plays concatenated into a single text file.\n",
        "\n",
        "It is encoded with Byte-Pair Encoding (BPE) that builds a vocabulary of subword units to optimally represent the input data. The encoded tokens for each split (train/val) are saved into corresponding binary files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8456a267-e0e0-418b-ac7f-62ea31ce05bb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8456a267-e0e0-418b-ac7f-62ea31ce05bb",
        "outputId": "3afefd88-8061-49eb-9192-d3f64b826fc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train has 301,966 tokens\n",
            "val has 36,059 tokens\n"
          ]
        }
      ],
      "source": [
        "input_file_path = os.path.join(os.path.abspath(''), 'input.txt')\n",
        "if not os.path.exists(input_file_path):\n",
        "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "    with open(input_file_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(requests.get(data_url).text)\n",
        "\n",
        "with open(input_file_path, 'r', encoding='utf-8') as f:\n",
        "    data = f.read()\n",
        "n = len(data)\n",
        "train_data = data[:int(n*0.9)]\n",
        "val_data = data[int(n*0.9):]\n",
        "\n",
        "# encode with tiktoken gpt2 bpe\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "train_ids = enc.encode_ordinary(train_data)\n",
        "val_ids = enc.encode_ordinary(val_data)\n",
        "print(f\"train has {len(train_ids):,} tokens\")\n",
        "print(f\"val has {len(val_ids):,} tokens\")\n",
        "\n",
        "# export to bin files\n",
        "train_ids = np.array(train_ids, dtype=np.uint16)\n",
        "val_ids = np.array(val_ids, dtype=np.uint16)\n",
        "train_ids.tofile(os.path.join(os.path.abspath(''), 'train.bin'))\n",
        "val_ids.tofile(os.path.join(os.path.abspath(''), 'val.bin'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3ea99ab-00f8-4172-bb00-68296fcb9c98",
      "metadata": {
        "id": "c3ea99ab-00f8-4172-bb00-68296fcb9c98"
      },
      "source": [
        "### Train a small GPT model from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c97463f9-105c-4495-b0e2-999b7906a775",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c97463f9-105c-4495-b0e2-999b7906a775",
        "outputId": "bb60c6a6-a270-4389-d809-0b9258d618e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[.] cuda chosen as device\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# default config values designed to train a gpt2 (124M) on OpenWebText\n",
        "# I/O\n",
        "\n",
        "init_from = 'scratch'  # 'scratch' or 'resume' or 'gpt2*'\n",
        "out_dir = 'out-shakespeare'\n",
        "eval_interval = 100\n",
        "eval_iters = 100\n",
        "log_interval = 20\n",
        "always_save_checkpoint = True  # if True, always save a checkpoint after each eval\n",
        "\n",
        "# data\n",
        "dataset = 'shakespeare'\n",
        "gradient_accumulation_steps = 5 * 8  # used to simulate larger batch sizes\n",
        "batch_size = 12  # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
        "block_size = 128\n",
        "\n",
        "# model\n",
        "# ------------------------------------------------------------------------------\n",
        "# play with these parameters! if you have access to the GPU runtime, make the model\n",
        "# bigger, it has a significant influence on its performance\n",
        "n_layer = 6\n",
        "n_head = 4\n",
        "n_embd = 128\n",
        "# ------------------------------------------------------------------------------\n",
        "dropout = 0.0  # for pretraining 0 is good, for finetuning try 0.1+\n",
        "bias = False  # do we use bias inside LayerNorm and Linear layers?\n",
        "\n",
        "# adamw optimizer\n",
        "learning_rate = 6e-4  # max learning rate\n",
        "max_iters = 500  # total number of training iterations\n",
        "weight_decay = 1e-1\n",
        "beta1 = 0.9\n",
        "beta2 = 0.95\n",
        "grad_clip = 1.0  # clip gradients at this value, or disable if == 0.0\n",
        "\n",
        "# learning rate decay settings\n",
        "decay_lr = True  # whether to decay the learning rate\n",
        "warmup_iters = 10  # how many steps to warm up for\n",
        "lr_decay_iters = 500  # should be ~= max_iters per Chinchilla\n",
        "min_lr = 6e-5  # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
        "\n",
        "# system\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
        "dtype = 'float16'  # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "compile = True  # use PyTorch 2.0 to compile the model to be faster\n",
        "print(f'[.] {device} chosen as device')\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "config_keys = [k for k, v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
        "config = {k: globals()[k] for k in config_keys}\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split, block_size=block_size)\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "# learning rate decay scheduler (cosine with warmup)\n",
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coeff ranges 0..1\n",
        "\n",
        "    return min_lr + coeff * (learning_rate - min_lr)\n",
        "\n",
        "\n",
        "def get_batch(split, batch_size=16, block_size=1024):\n",
        "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
        "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
        "    if split == 'train':\n",
        "        data = np.memmap(os.path.join(os.path.abspath(''), 'train.bin'),\n",
        "                         dtype=np.uint16, mode='r')\n",
        "    else:\n",
        "        data = np.memmap(os.path.join(os.path.abspath(''), 'val.bin'),\n",
        "                         dtype=np.uint16, mode='r')\n",
        "\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i + block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i + 1:i + 1 + block_size]).astype(np.int64)) for i in ix])\n",
        "    if device_type == 'cuda':\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4c639e8-63b6-4bc4-87f0-0f7569922d11",
      "metadata": {
        "id": "f4c639e8-63b6-4bc4-87f0-0f7569922d11"
      },
      "source": [
        "Various inits, derived attributes, I/O setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ab55f13-8d25-440c-8894-90090317ce59",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ab55f13-8d25-440c-8894-90090317ce59",
        "outputId": "550733c5-bc45-49ae-e842-e611d0e9f6bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens per iteration will be: 61,440\n",
            "Initializing a new model from scratch\n",
            "defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\n",
            "number of parameters: 7.62M\n",
            "num decayed parameter tensors: 26, with 7,634,944 parameters\n",
            "num non-decayed parameter tensors: 13, with 1,664 parameters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-4-2c667275b9d5>:53: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n"
          ]
        }
      ],
      "source": [
        "seed_offset = 0\n",
        "tokens_per_iter = gradient_accumulation_steps * batch_size * block_size\n",
        "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
        "\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "torch.manual_seed(1337 + seed_offset)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True  # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True  # allow tf32 on cudnn\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu'  # for later use in torch.autocast\n",
        "\n",
        "# note: float16 data type will automatically use a GradScaler\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "\n",
        "data_dir = os.path.join('data', dataset)\n",
        "\n",
        "# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n",
        "iter_num = 0\n",
        "best_val_loss = 1e9\n",
        "\n",
        "# attempt to derive vocab_size from the dataset\n",
        "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
        "meta_vocab_size = None\n",
        "if os.path.exists(meta_path):\n",
        "    with open(meta_path, 'rb') as f:\n",
        "        meta = pickle.load(f)\n",
        "    meta_vocab_size = meta['vocab_size']\n",
        "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
        "\n",
        "# model init\n",
        "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
        "                  bias=bias, vocab_size=None, dropout=dropout)\n",
        "\n",
        "# init a new model from scratch\n",
        "print(\"Initializing a new model from scratch\")\n",
        "# determine the vocab size we'll use for from-scratch training\n",
        "if meta_vocab_size is None:\n",
        "    print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n",
        "\n",
        "model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
        "gptconf = GPTConfig(**model_args)\n",
        "model = GPT(gptconf)\n",
        "\n",
        "# crop down the model block size if desired, using model surgery\n",
        "if block_size < model.config.block_size:\n",
        "    model.crop_block_size(block_size)\n",
        "    model_args['block_size'] = block_size  # so that the checkpoint will have the right value\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
        "\n",
        "# optimizer\n",
        "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
        "if init_from == 'resume':\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "checkpoint = None  # free up memory\n",
        "\n",
        "# compile the model\n",
        "if compile:\n",
        "    print(\"compiling the model... (takes a ~minute)\")\n",
        "    unoptimized_model = model\n",
        "    model = torch.compile(model)  # requires PyTorch 2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "099b0d9b-e5fb-4a8b-a5e9-24929a4904af",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "099b0d9b-e5fb-4a8b-a5e9-24929a4904af",
        "outputId": "954bc737-403e-4e9d-dcac-85ad1ad48888"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss 10.8287, val loss 10.8227\n",
            "iter 0: loss 10.8279, time 20081.17ms\n",
            "iter 20: loss 9.0177, time 488.34ms\n",
            "iter 40: loss 7.1812, time 490.91ms\n",
            "iter 60: loss 6.4597, time 494.55ms\n",
            "iter 80: loss 5.9980, time 496.67ms\n",
            "step 100: train loss 5.7278, val loss 5.8724\n",
            "saving checkpoint to out-shakespeare\n",
            "iter 100: loss 5.8159, time 1858.18ms\n",
            "iter 120: loss 5.2991, time 504.68ms\n",
            "iter 140: loss 5.1239, time 501.93ms\n",
            "iter 160: loss 5.0640, time 506.78ms\n",
            "iter 180: loss 4.9745, time 496.50ms\n",
            "step 200: train loss 4.7684, val loss 5.1917\n",
            "saving checkpoint to out-shakespeare\n",
            "iter 200: loss 4.9746, time 1853.07ms\n",
            "iter 220: loss 4.7539, time 496.29ms\n",
            "iter 240: loss 4.5990, time 496.75ms\n",
            "iter 260: loss 4.6500, time 499.99ms\n",
            "iter 280: loss 4.5809, time 500.66ms\n",
            "step 300: train loss 4.3920, val loss 5.0102\n",
            "saving checkpoint to out-shakespeare\n",
            "iter 300: loss 4.3941, time 1962.26ms\n",
            "iter 320: loss 4.4137, time 499.32ms\n",
            "iter 340: loss 4.1362, time 499.07ms\n",
            "iter 360: loss 4.6038, time 497.47ms\n",
            "iter 380: loss 4.2089, time 497.76ms\n",
            "step 400: train loss 4.2401, val loss 4.9247\n",
            "saving checkpoint to out-shakespeare\n",
            "iter 400: loss 4.2205, time 1918.63ms\n",
            "iter 420: loss 4.1604, time 504.53ms\n",
            "iter 440: loss 3.9604, time 496.66ms\n",
            "iter 460: loss 4.0116, time 497.44ms\n",
            "iter 480: loss 4.0913, time 497.96ms\n",
            "step 500: train loss 4.1479, val loss 4.9101\n",
            "saving checkpoint to out-shakespeare\n",
            "iter 500: loss 4.0882, time 1891.43ms\n"
          ]
        }
      ],
      "source": [
        "# training loop\n",
        "X, Y = get_batch('train', block_size=block_size)  # fetch the very first batch\n",
        "t0 = time.time()\n",
        "local_iter_num = 0  # number of iterations in the lifetime of this process\n",
        "\n",
        "while True:\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # evaluate the loss on train/val sets and write checkpoints\n",
        "    if iter_num % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
        "            best_val_loss = losses['val']\n",
        "            if iter_num > 0:\n",
        "                checkpoint = {\n",
        "                    'model': model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'model_args': model_args,\n",
        "                    'iter_num': iter_num,\n",
        "                    'best_val_loss': best_val_loss,\n",
        "                    'config': config,\n",
        "                }\n",
        "                print(f\"saving checkpoint to {out_dir}\")\n",
        "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
        "\n",
        "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
        "    # and using the GradScaler if data type is float16\n",
        "    for micro_step in range(gradient_accumulation_steps):\n",
        "        with ctx:\n",
        "            logits, loss = model(X, Y)\n",
        "            loss = loss / gradient_accumulation_steps  # scale the loss to account for gradient accumulation\n",
        "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
        "        X, Y = get_batch('train', block_size=block_size)\n",
        "        # backward pass, with gradient scaling if training in fp16\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "    # clip the gradient\n",
        "    if grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "\n",
        "    # step the optimizer and scaler if training in fp16\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    # flush the gradients as soon as we can, no need for this memory anymore\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # timing and logging\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "    if iter_num % log_interval == 0:\n",
        "        # get loss as float. note: this is a CPU-GPU sync point\n",
        "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
        "        lossf = loss.item() * gradient_accumulation_steps\n",
        "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt * 1000:.2f}ms\")\n",
        "\n",
        "    iter_num += 1\n",
        "    local_iter_num += 1\n",
        "\n",
        "    # termination conditions\n",
        "    if iter_num > max_iters:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GHjeuVIpXdPJ",
      "metadata": {
        "id": "GHjeuVIpXdPJ"
      },
      "source": [
        "> In case you are having trouble with securing a GPU runtime and training the model, download the trained weights from [here](https://drive.google.com/file/d/17gfJ76SyGJVW3jinz3Xv5B7XxTyE9NNA/view?usp=sharing). Create the directory called `out-shakespeare` and place the downloaded weights there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uyoXtuGaggRL",
      "metadata": {
        "id": "uyoXtuGaggRL"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('/content/out-shakespeare'):\n",
        "  os.makedirs('/content/out-shakespeare')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C8FZY_5Abirw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8FZY_5Abirw",
        "outputId": "4c553571-1cd1-45d0-d14e-16363648721d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-03-17 11:05:09--  https://docs.google.com/uc?export=download&id=17gfJ76SyGJVW3jinz3Xv5B7XxTyE9NNA\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.68.102, 74.125.68.139, 74.125.68.100, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.68.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=17gfJ76SyGJVW3jinz3Xv5B7XxTyE9NNA&export=download [following]\n",
            "--2025-03-17 11:05:09--  https://drive.usercontent.google.com/download?id=17gfJ76SyGJVW3jinz3Xv5B7XxTyE9NNA&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 172.217.194.132, 2404:6800:4003:c04::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|172.217.194.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 91684437 (87M) [application/octet-stream]\n",
            "Saving to: ‘/content/out-shakespeare/ckpt.pt’\n",
            "\n",
            "/content/out-shakes 100%[===================>]  87.44M   186MB/s    in 0.5s    \n",
            "\n",
            "2025-03-17 11:05:18 (186 MB/s) - ‘/content/out-shakespeare/ckpt.pt’ saved [91684437/91684437]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=17gfJ76SyGJVW3jinz3Xv5B7XxTyE9NNA' -O /content/out-shakespeare/ckpt.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77bf31a0-f17a-4103-86d7-e189cc12da5b",
      "metadata": {
        "id": "77bf31a0-f17a-4103-86d7-e189cc12da5b"
      },
      "source": [
        "---\n",
        "### Sample from a trained model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37ba4440-2dcc-461a-bf87-75a7607bf389",
      "metadata": {
        "id": "37ba4440-2dcc-461a-bf87-75a7607bf389"
      },
      "source": [
        "Initialize the trained model from a directory."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0f0edb2-2df8-47d6-8cc9-3b8737baaede",
      "metadata": {
        "id": "a0f0edb2-2df8-47d6-8cc9-3b8737baaede"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\"><b>Tip:</b> There is no need to re-initialize the model and load it into memory again if you've just trained it. Run the next cell if the context of the notebook was reset after the training.</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5a63e56-9ce0-4738-a800-f907b93168a3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5a63e56-9ce0-4738-a800-f907b93168a3",
        "outputId": "c66e220a-3851-4b3a-d406-bfa071cd9282"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of parameters: 7.62M\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "init_from = 'resume'\n",
        "out_dir = 'out-shakespeare' # ignored if init_from is not 'resume'\n",
        "\n",
        "seed = 1337\n",
        "dtype = 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
        "compile = True # use PyTorch 2.0 to compile the model to be faster\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "\n",
        "\n",
        "# init from a model saved in a specific directory\n",
        "ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
        "checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "gptconf = GPTConfig(**checkpoint['model_args'])\n",
        "model = GPT(gptconf)\n",
        "state_dict = checkpoint['model']\n",
        "unwanted_prefix = '_orig_mod.'\n",
        "for k,v in list(state_dict.items()):\n",
        "    if k.startswith(unwanted_prefix):\n",
        "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "if compile:\n",
        "    model = torch.compile(model) # requires PyTorch 2.0 (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23b13972-f456-49e4-a59a-afa2d4fb721c",
      "metadata": {
        "id": "23b13972-f456-49e4-a59a-afa2d4fb721c"
      },
      "outputs": [],
      "source": [
        "# assume gpt-2 encodings by default\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
        "decode = lambda l: enc.decode(l)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78429441-dda3-4ae9-927f-3e00cff0b444",
      "metadata": {
        "id": "78429441-dda3-4ae9-927f-3e00cff0b444"
      },
      "source": [
        "---\n",
        "### Sample from the trained model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "810fa614-cdb1-49d9-9dfe-64df0d35efd8",
      "metadata": {
        "id": "810fa614-cdb1-49d9-9dfe-64df0d35efd8"
      },
      "source": [
        "We can prompt the model by providing a context. Try sampling with a different context and sample length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5b4914a-76c6-4554-b5df-f93eff268d32",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5b4914a-76c6-4554-b5df-f93eff268d32",
        "outputId": "6b66f06d-3bd7-493d-b61a-ff01822c2293",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Universe is vast-shented--\n",
            "I will have we be for so his son\n",
            "Of no heart in our person, as my heart,\n",
            "I'll die!\n",
            "\n",
            "COMINIUS:\n",
            "To help him.\n",
            "\n",
            "LADY:\n",
            "Nay, who thou hast thou duke he would not, I shall my husband, and dead;\n",
            "Though she is the common kinsman:\n",
            "Was she be seen a little with a thousand better name call me.\n",
            "\n",
            "HENRY VI:\n",
            "The other thing to say I have'st of nothing with war more.\n",
            "\n",
            "KING RICHARD II:\n",
            "F\n",
            "==============================\n",
            "The Universe is vast!\n",
            "\n",
            "NORTHUMLAND:\n",
            "Take it is the court is the foe, you,\n",
            "And, my cousin,\n",
            "This is the second world of the grave and my dear lord.\n",
            "\n",
            "\n",
            "JULI am done?\n",
            "\n",
            "KING EDWARD:\n",
            "Y:\n",
            "I am, my lord, madam, good time may say, if he's one, I will be mine.\n",
            "\n",
            "BUCKINGHAM:\n",
            "My name as his fortune with the people,\n",
            "But for you say, as the world, and that; and lay us?\n",
            "\n",
            "ES:\n",
            "In it be the queen of\n",
            "==============================\n",
            "The Universe is vast\n",
            "To do here in thy heart's very face'd,\n",
            "And, to the kents?\n",
            "\n",
            "Nay, even that we swear, though I be a life,\n",
            "Here\n",
            "I'll have of the night's time by the way,\n",
            "Will be a more more than the rest?\n",
            "I will give me so, good, nor a man;\n",
            "His good friend; here not be so, come, there were.\n",
            "\n",
            "First Senator:\n",
            "Which in the city is the queen, my lord, say but ever thou'll be a king,\n",
            "That comes the Capitol.\n",
            "\n",
            "KING RICHARD III\n",
            "==============================\n",
            "The Universe is vast and by the Duke of England's hand enough;\n",
            "More than you be on my lord;\n",
            "And then in my sovereign:\n",
            "As for you dost thou he have a king, an the crown,\n",
            "And, but a friar, and then never will and wouldst meet,\n",
            "And, a best can know the house, what I will, for it not in me.\n",
            "\n",
            "POM IV:\n",
            "That't she was in each means.\n",
            "\n",
            "GLOUCESTER:\n",
            "This is all the city, good, and so; I follow me:\n",
            "For you are they have heard you, at\n",
            "==============================\n",
            "The Universe is vast, I will to me.\n",
            "\n",
            "NORTHUMCES:\n",
            "I'll hear you have not her, sir, but,\n",
            "Thou comes the king.\n",
            "\n",
            "KING RICHARD II:\n",
            "Thou hast thou hast not.\n",
            "\n",
            "Hath?\n",
            "\n",
            "KING EDWARD IV:\n",
            "My lord, or well is a traitor, my life and I both be--\n",
            "\n",
            "KING RICHARD III:\n",
            "Not when 'tis a sea, my arms, that hadst thoust:\n",
            "Thy and God, but bid any more love?\n",
            "\n",
            "QUEENI am thou mayst thou art\n",
            "==============================\n"
          ]
        }
      ],
      "source": [
        "# encode the beginning of the prompt\n",
        "context = 'The Universe is vast'\n",
        "start_ids = encode(context)\n",
        "num_samples = 5\n",
        "sample_len = 128\n",
        "\n",
        "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    with ctx:\n",
        "        for k in range(num_samples):\n",
        "            x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
        "\n",
        "            probs, y = model.generate(x, sample_len, temperature=temperature, top_k=top_k)\n",
        "            print(decode(y[0].tolist()))\n",
        "            print('==============================')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kkIxQd7JhKTW",
      "metadata": {
        "id": "kkIxQd7JhKTW"
      },
      "source": [
        "The generated text mostly does not make sense but the model could definitely capture some attributes of the Shakespearean style."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c2334be-c4a4-44b9-a8b4-ab74f61a75a6",
      "metadata": {
        "id": "8c2334be-c4a4-44b9-a8b4-ab74f61a75a6"
      },
      "source": [
        "Let's run the model on the validation dataset. Experiment with the context size and see how it influences the generation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37397a23-1d30-492d-9bfb-b56bfae0c1b5",
      "metadata": {
        "id": "37397a23-1d30-492d-9bfb-b56bfae0c1b5"
      },
      "source": [
        "> Note: the context, provided to the model, consists of the first few tokens from each sample. Be shure to exclude is from evaluation as it will always be \"right\" and skew the metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fdad63a-4755-4629-b58e-c715bc878517",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fdad63a-4755-4629-b58e-c715bc878517",
        "outputId": "f623b2e2-b327-436d-fe07-2b4a69aa0609",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch 0/17\n",
            "batch 1/17\n",
            "batch 2/17\n",
            "batch 3/17\n",
            "batch 4/17\n",
            "batch 5/17\n",
            "batch 6/17\n",
            "batch 7/17\n",
            "batch 8/17\n",
            "batch 9/17\n",
            "batch 10/17\n",
            "batch 11/17\n",
            "batch 12/17\n",
            "batch 13/17\n",
            "batch 14/17\n",
            "batch 15/17\n",
            "batch 16/17\n"
          ]
        }
      ],
      "source": [
        "# experiment with sample length, context size and their influence on the evaluation metrics\n",
        "sample_len = 64\n",
        "batch_size = 32\n",
        "start_len = 5\n",
        "\n",
        "temperature = 0.8\n",
        "top_k = 200\n",
        "# -------------------------------------\n",
        "\n",
        "val_data = np.memmap('./val.bin', dtype=np.uint16, mode='r')\n",
        "num_batches = len(val_data) // sample_len // batch_size\n",
        "\n",
        "pred_sent = []\n",
        "gt_sent = []\n",
        "\n",
        "pred_tokens = []\n",
        "gt_tokens = []\n",
        "\n",
        "pred_probs = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    with ctx:\n",
        "        for batch_i in range(num_batches):\n",
        "          print(f'batch {batch_i}/{num_batches}')\n",
        "\n",
        "          X_val, _ = get_batch('val', batch_size, sample_len)\n",
        "\n",
        "          for k in range(batch_size):\n",
        "              start_ids = X_val[k, :start_len]\n",
        "              # print('START:', start_ids, ' - \"', decode(list(start_ids)), '\"')\n",
        "              x = start_ids.clone().detach().type(torch.long).to(device)[None, ...]\n",
        "\n",
        "              probs, pred = model.generate(x, sample_len-start_len, temperature=temperature, top_k=top_k)\n",
        "              pred_probs.append(torch.cat(probs).cpu())\n",
        "\n",
        "              # skip the \"context\" that was provided\n",
        "              decoded_pred = decode(pred[0, start_len:].tolist())\n",
        "              pred_sent.append(decoded_pred)\n",
        "\n",
        "              decoded_gt = decode(X_val[k, start_len:].tolist())\n",
        "              gt_sent.append(decoded_gt)\n",
        "\n",
        "              # print('PRED DECODED: ', decoded_pred)\n",
        "              # print('--------------------------')\n",
        "              # print('GT DECODED: ', decoded_gt)\n",
        "\n",
        "              gt_tokens.append([X_val[k, start_len:].cpu().numpy()])\n",
        "              pred_tokens.append(pred[0, start_len:].cpu().numpy())\n",
        "\n",
        "              # print('==============================')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YQpGIAFBtaNv",
      "metadata": {
        "id": "YQpGIAFBtaNv"
      },
      "source": [
        "Before proceeding, we will take a smaller subset of the output due to the computational limitations in Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "L7VeUrlvQEhC",
      "metadata": {
        "id": "L7VeUrlvQEhC"
      },
      "outputs": [],
      "source": [
        "pred_sent = pred_sent[:120]\n",
        "gt_sent = gt_sent[:120]\n",
        "gt_tokens = gt_tokens[:120]\n",
        "pred_tokens = pred_tokens[:120]\n",
        "pred_probs = pred_probs[:120]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae52098f-3052-428a-84b2-58f1ba01dddd",
      "metadata": {
        "id": "ae52098f-3052-428a-84b2-58f1ba01dddd"
      },
      "source": [
        "### Model evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e93d511-64b8-444e-b16b-245b572e7186",
      "metadata": {
        "id": "5e93d511-64b8-444e-b16b-245b572e7186"
      },
      "source": [
        "It is important to be able to quantitatively evaluate language models. Some of the popular evaluation metrics that use reference text are BLEU score, BERTScore and Perplexity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34c997b8-605f-450a-a416-b177d09473fc",
      "metadata": {
        "id": "34c997b8-605f-450a-a416-b177d09473fc"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\"><b>Tip:</b> Save the needed predictions while running the model on the validation dataset in the cell above. Computing the metrics on the dataset level is more straightforward.</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec94b8b2-7e63-48e8-a1f3-32f5400a6970",
      "metadata": {
        "id": "ec94b8b2-7e63-48e8-a1f3-32f5400a6970"
      },
      "source": [
        "#### 1. BLEU score\n",
        "\n",
        "The BLEU (Bilingual Evaluation Understudy) score works by comparing the n-grams (contiguous sequences of n tokens) in the generated text to those in the reference text(s). It calculates a precision score for each n-gram size (typically up to 4-grams) and combines these scores using a weighted geometric mean."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e664a9a4-43ed-4e6e-afd9-d7ac3732e673",
      "metadata": {
        "id": "e664a9a4-43ed-4e6e-afd9-d7ac3732e673"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\"><b>Challenge 1:</b> Use the NLTK framework to calculate BLEU scores for 1-, 2-, 3-, and 4-grams on the validation dataset.</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "260230b3-759d-425f-9ba9-8bc1fefc6a90",
      "metadata": {
        "id": "260230b3-759d-425f-9ba9-8bc1fefc6a90"
      },
      "outputs": [],
      "source": [
        "...\n",
        "\n",
        "print('BLEU-1: ', bleu1)\n",
        "print('BLEU-2: ', bleu2)\n",
        "print('BLEU-3: ', bleu3)\n",
        "print('BLEU-4: ', bleu4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce5d37fa-f010-4ee2-8036-2114b178487c",
      "metadata": {
        "id": "ce5d37fa-f010-4ee2-8036-2114b178487c"
      },
      "source": [
        "#### 2. BERTScore\n",
        "\n",
        "BERTScore  leverages contextual embeddings from BERT (Bidirectional Encoder Representations from Transformers) to compute similarity between sentences or text spans. Unlike BLEU score that works on a token level, it considers both word overlap and contextual information, providing a more accurate evaluation.\n",
        "\n",
        "Compared to BLEU score, BERTScore offers several advantages:\n",
        "\n",
        "- Contextual understanding: BERTScore considers the contextual meaning of words, capturing nuances that BLEU, which relies solely on word overlap, may miss.\n",
        "- Robustness to word order: BERTScore's contextual embeddings enable it to handle variations in word order, making it more robust to changes in sentence structure or word arrangement.\n",
        "- Higher correlation with human judgment: BERTScore has been shown to correlate better with human judgment in evaluating text quality, especially in tasks like summarization and text generation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba7609a3-43b8-4956-9175-818b655d5313",
      "metadata": {
        "id": "ba7609a3-43b8-4956-9175-818b655d5313"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\"><b>Challenge 2:</b> Use the bert_score package to calculate BERTScore on the validation dataset.</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "691944c9-c086-4ffc-bf7d-61f378b22172",
      "metadata": {
        "id": "691944c9-c086-4ffc-bf7d-61f378b22172"
      },
      "outputs": [],
      "source": [
        "...\n",
        "\n",
        "print(f\"BERTScore Precision: {bs_precision:.4f}, Recall: {bs_recall:.4f}, F1: {bs_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bc5b48b-beb1-490c-a997-de7610185194",
      "metadata": {
        "id": "5bc5b48b-beb1-490c-a997-de7610185194"
      },
      "source": [
        "#### 3. Perplexity\n",
        "\n",
        "Perplexity is a measurement used in natural language processing (NLP) to assess how well a language model predicts a sample of text. It quantifies the average uncertainty or surprise of the model in predicting the next word or token in a sequence. Lower perplexity values indicate that the model is more confident and accurate in its predictions, while higher values suggest more uncertainty."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84440291-2467-4294-a041-d448c053b6b5",
      "metadata": {
        "id": "84440291-2467-4294-a041-d448c053b6b5"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\"><b>Challenge 3:</b> Use the torcheval package to calculate perplexity on the validation dataset.</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b28cb09d-3cdd-4d8a-b0fc-0727a9e0cafa",
      "metadata": {
        "id": "b28cb09d-3cdd-4d8a-b0fc-0727a9e0cafa"
      },
      "outputs": [],
      "source": [
        "...\n",
        "\n",
        "print('Perplexity: ', perplexity)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp24",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
